{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPfS0XAejIrYn19OyQexk3F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UAPH451551/PH451_551_Sp24/blob/main/Exercises/08B_rnns_and_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hands On #8B\n",
        "\n",
        "**RNNs, Attention, and Transformers**"
      ],
      "metadata": {
        "id": "bPYku5S2bZf4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_5rT9hCW_Yu"
      },
      "source": [
        "File name convention: For group 42 and memebers Richard Stallman and Linus <br>\n",
        "Torvalds it would be <br>\n",
        "\"08B_group42_Stallman_Torvalds.pdf\".\n",
        "\n",
        "Submission via blackboard (UA).\n",
        "\n",
        "Feel free to answer free text questions in text cells using markdown and <br>\n",
        "possibly $\\LaTeX{}$ if you want to.\n",
        "\n",
        "**You don't have to understand every line of code here and it is not intended <br>\n",
        "for you to try to understand every line of code.   <br>\n",
        "Big blocks of code are usually meant to just be clicked through.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)"
      ],
      "metadata": {
        "id": "YWgUpTD9Rxac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "51q5DRePxDJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and Preparing the Dataset"
      ],
      "metadata": {
        "id": "Qme7akjZPzP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_time_series(batch_size, n_steps):\n",
        "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
        "    time = np.linspace(0, 1, n_steps)\n",
        "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  #   wave 1\n",
        "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2\n",
        "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # + noise\n",
        "    return series[..., np.newaxis].astype(np.float32)"
      ],
      "metadata": {
        "id": "qLie-1cRsPAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 1)\n",
        "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
        "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
        "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]"
      ],
      "metadata": {
        "id": "jzdTaLissSv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, y_train.shape"
      ],
      "metadata": {
        "id": "dqiEg16esU4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_series(series, y=None, y_pred=None, x_label=\"$t$\", y_label=\"$x(t)$\", legend=True):\n",
        "    plt.plot(series, \".-\")\n",
        "    if y is not None:\n",
        "        plt.plot(n_steps, y, \"bo\", label=\"Target\")\n",
        "    if y_pred is not None:\n",
        "        plt.plot(n_steps, y_pred, \"rx\", markersize=10, label=\"Prediction\")\n",
        "    plt.grid(True)\n",
        "    if x_label:\n",
        "        plt.xlabel(x_label, fontsize=16)\n",
        "    if y_label:\n",
        "        plt.ylabel(y_label, fontsize=16, rotation=0)\n",
        "    plt.hlines(0, 0, 100, linewidth=1)\n",
        "    plt.axis([0, n_steps + 1, -1, 1])\n",
        "    if legend and (y or y_pred):\n",
        "        plt.legend(fontsize=14, loc=\"upper left\")\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3, sharey=True, figsize=(12, 4))\n",
        "for col in range(3):\n",
        "    plt.sca(axes[col])\n",
        "    plot_series(X_valid[col, :, 0], y_valid[col, 0],\n",
        "                y_label=(\"$x(t)$\" if col==0 else None),\n",
        "                legend=(col == 0))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UBMBEznYsYkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeSeries(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X.copy()).float()\n",
        "        self.y = torch.from_numpy(y.copy()).float()\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_data = TimeSeries(X_train, y_train)\n",
        "valid_data = TimeSeries(X_valid, y_valid)\n",
        "test_data = TimeSeries(X_test, y_test)\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
        "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "mKL8RAgNvbhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_validate(train_loader, val_loader, model, optimizer, criterion, num_epochs, metric=None, scheduler=None, device='cpu'):\n",
        "    history = {\n",
        "        'epoch': [],\n",
        "        'train_loss': [],\n",
        "        'train_metric': [],\n",
        "        'val_loss': [],\n",
        "        'val_metric': [],\n",
        "        'learning_rate': []\n",
        "    }  # Initialize a dictionary to store epoch-wise results\n",
        "\n",
        "    model.to(device)  # Move the model to the specified device\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        epoch_loss = 0.0  # Initialize the epoch loss and metric values\n",
        "        epoch_metric = 0.0\n",
        "\n",
        "        # Training loop\n",
        "        for X, y in train_loader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            y = y\n",
        "            optimizer.zero_grad()  # Clear existing gradients\n",
        "            outputs = model(X)  # Make predictions\n",
        "            loss = criterion(outputs, y)  # Compute the loss\n",
        "            loss.backward()  # Compute gradients\n",
        "            optimizer.step()  # Update model parameters\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # THESE LINES HAVE BEEN UPDATED TO ACCOUNT FOR DEFAULT ARGUMENTS\n",
        "            if metric is not None:\n",
        "                epoch_metric += metric(outputs, y)\n",
        "            else:\n",
        "                epoch_metric += 0.0\n",
        "\n",
        "        # Average training loss and metric\n",
        "        epoch_loss /= len(train_loader)\n",
        "        epoch_metric /= len(train_loader)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        with torch.no_grad():  # Disable gradient calculation\n",
        "            val_loss = 0.0\n",
        "            val_metric = 0.0\n",
        "            for X_val, y_val in val_loader:\n",
        "                X_val = X_val.to(device)\n",
        "                y_val = y_val.to(device)\n",
        "                y_val = y_val\n",
        "                outputs_val = model(X_val)  # Make predictions\n",
        "                val_loss += criterion(outputs_val, y_val).item()  # Compute loss\n",
        "                if metric is not None:\n",
        "                    val_metric += metric(outputs_val, y_val)\n",
        "                else:\n",
        "                    val_metric += 0.0\n",
        "\n",
        "            val_loss /= len(val_loader)\n",
        "            val_metric /= len(val_loader)\n",
        "\n",
        "        # Append epoch results to history\n",
        "        history['epoch'].append(epoch)\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "        history['train_metric'].append(epoch_metric)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_metric'].append(val_metric)\n",
        "        history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, '\n",
        "              f'Train Metric: {epoch_metric:.4f}, Val Loss: {val_loss:.4f}, '\n",
        "              f'Val Metric: {val_metric:.4f}')\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "    return history, model"
      ],
      "metadata": {
        "id": "9jHIqE2YwLyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, data_loader, criterion, metric=None, device='cpu'):\n",
        "    model.to(device)  # Move the model to the specified device\n",
        "\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    total_loss = 0.0  # Initialize the total loss and metric values\n",
        "    total_metric = 0.0\n",
        "\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient tracking\n",
        "        for batch in data_loader:\n",
        "            X, y = batch\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            # Pass the data to the model and make predictions\n",
        "            outputs = model(X)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "            # Add the loss and metric for the batch to the total values\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if metric is not None:\n",
        "                total_metric += metric(outputs, y)\n",
        "            else:\n",
        "                total_metric += 0.0\n",
        "\n",
        "    # Average loss and metric for the entire dataset\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    avg_metric = total_metric / len(data_loader)\n",
        "\n",
        "    print(f'Test Loss: {avg_loss:.4f}, Test Metric: {avg_metric:.4f}')\n",
        "\n",
        "    return avg_loss, avg_metric"
      ],
      "metadata": {
        "id": "y_AVwhxzwNKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Networks"
      ],
      "metadata": {
        "id": "Nnn8DZ8KQ-eK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Simple Recurrent Neural Network RNN\n",
        "\n",
        "The simplest form of **RNN** one can build **takes an output and passes it into the** <br>\n",
        "**next input**. The idea is that, if your data has some form of **sequential** property <br>\n",
        "such as being elements of a **time series** or a sentence or even generally related <br>\n",
        "(not explicitly sequential) values, you should attempt to **inform your model via** <br>\n",
        "**explicit neural connections** that those values are linked. By passing the output <br>\n",
        "of one neuron to the next neuron in a sequence, you can explicitly inform your <br>\n",
        "model of the relationship of an input to its neighboring inputs."
      ],
      "metadata": {
        "id": "tC81tARAS0yY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Simple RNN\n",
        "\n",
        "Build  a simple RNN using the `SimpleRNN` module. Your model should be a <br>\n",
        "Sequential model called `simplernn` of the following form:<br>\n",
        "`input_size=1, hidden_size=16, output_size=1, seq_len=50`\n",
        "\n",
        "Train the model using the train_and_validate function on train_loader and <br>\n",
        "valid_loader using MSELoss as criterion, L1Loss as metric, and the Adam <br>\n",
        "optimizer with lr=0.001. Train for 6 epochs."
      ],
      "metadata": {
        "id": "bu-O5kL7w7PQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ],
      "metadata": {
        "id": "qhl_mpxPyMIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, seq_len):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.rnn1 = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.rnn2 = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size * seq_len, output_size)\n",
        "    def forward(self, x):\n",
        "        x, hn = self.rnn1(x)\n",
        "        x, _ = self.rnn2(x, hn)\n",
        "        x = x.reshape(-1, x.size(1) * x.size(2))\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "7W_zkdEKyvd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simplernn = SimpleRNN(\n",
        "# criterion =\n",
        "# metric =\n",
        "# optimizer =\n",
        "# history, simplernn = train_and_validate("
      ],
      "metadata": {
        "id": "f_IY8OsL_a2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ],
      "metadata": {
        "id": "DsqLAiqTyTfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(simplernn, test_loader, criterion, metric=metric, device=device)"
      ],
      "metadata": {
        "id": "axqofEdK0KVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_curves(loss, val_loss):\n",
        "    plt.plot(np.arange(len(loss)) + 0.5, loss, \"b.-\", label=\"Training loss\")\n",
        "    plt.plot(np.arange(len(val_loss)) + 1, val_loss, \"r.-\", label=\"Validation loss\")\n",
        "    plt.gca().xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
        "    plt.axis([1, 10, 0.002, 0.01])\n",
        "    plt.legend(fontsize=14)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid(True)"
      ],
      "metadata": {
        "id": "NLtV-xKt0viV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curves(history[\"train_loss\"], history[\"val_loss\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FfzfB3Kp0mcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = simplernn(torch.from_numpy(X_test).to(device)).to('cpu').detach().numpy()\n",
        "plot_series(X_test[0, :, 0], y_test[0, 0], y_pred[0, 0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G4QBEMeb0qaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred.shape)"
      ],
      "metadata": {
        "id": "J0qYfkYPIZRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(43) # not 42, as it would give the first series in the train set\n",
        "\n",
        "new_series = generate_time_series(1, n_steps + 10)\n",
        "X_new, Y_new = new_series[:, :n_steps], new_series[:, n_steps:]"
      ],
      "metadata": {
        "id": "T-MHM_8GkYIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X_new\n",
        "for step_ahead in range(10):\n",
        "    y_pred_one = simplernn(torch.from_numpy(X[:, step_ahead:]).to(device)).to('cpu').detach().numpy()[:, np.newaxis, :]\n",
        "    X = np.concatenate([X, y_pred_one], axis=1)\n",
        "\n",
        "y_pred_new = X[:, n_steps:]"
      ],
      "metadata": {
        "id": "5CYO4eb_74sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_new.shape"
      ],
      "metadata": {
        "id": "TqrMpy2H78RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_multiple_forecasts(X, Y, Y_pred):\n",
        "    n_steps = X.shape[1]\n",
        "    ahead = Y.shape[1]\n",
        "    plot_series(X[0, :, 0])\n",
        "    plt.plot(np.arange(n_steps, n_steps + ahead), Y[0, :, 0], \"bo-\", label=\"Actual\")\n",
        "    plt.plot(np.arange(n_steps, n_steps + ahead), Y_pred[0, :, 0], \"rx-\", label=\"Forecast\", markersize=10)\n",
        "    plt.axis([0, n_steps + ahead, -1, 1])\n",
        "    plt.legend(fontsize=14)"
      ],
      "metadata": {
        "id": "_5m0f60G77mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_multiple_forecasts(X_new, Y_new, y_pred_new)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XxxMhNLb7ztQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Gated Recurrent Unit (GRU)\n",
        "\n",
        "GRU models are **actually very new** in the timeline of RNNs having been first <br>\n",
        "**proposed in 2014** by Kyunghyun Cho et al. The main feature that this adds to the <br>\n",
        "RNN architechture is that it replaces the simple densely connected unit with a <br>\n",
        "new unit that adds a **\"forget gate\"**. You might remember the **characteristic** <br>\n",
        "**equation for a standard neuron** is $a_i=\\sigma(w_ix_i + b_l)$. For the GRU you add <br>\n",
        "a term which depends on a new weight we can call u which is applied to the <br>\n",
        "output of the previous neuron: $a_i=\\sigma(w_ix_i + u_ih_{i-1} + b_l)$ where h is the final <br>\n",
        "output of the previous neuron. Additionally, it takes that initial activation and <br>\n",
        "applies two steps:<br>\n",
        "1) $\\hat h_i=\\phi(w_{2i}x_i+u_{2i}(a_i\\odot h_{i-1}) + b_{2l})$ where $\\phi$ is tanh and $w_2$ is a second <br>\n",
        "neuron weight meaning that the simplest **GRU will have 4 weights and 2 biases**.<br>\n",
        "2) $h_i=(1-a_i)\\odot h_{i-1}+a_i\\odot \\hat h_i$)\n",
        "\n",
        "If we consider what those equations mean then you can probably work out that <br>\n",
        "**(1), our forget gate**, is checking how correlated our activation is with our <br>\n",
        "previous neuron's activation. Then, **if our output is highly correlated**, our <br>\n",
        "final activation **(2) will have a large contribution from our previous neuron** <br>\n",
        "and if uncorrelated, it will have a small contribution.\n",
        "\n",
        "The modern standard implementation of GRU also now includes a \"reset gate\" and <br>\n",
        "an \"update gate\" which you can read about further [here](https://en.wikipedia.org/wiki/Gated_recurrent_unit)."
      ],
      "metadata": {
        "id": "RLnjnt_gR2Iw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Simple GRU Model\n",
        "\n",
        "Build an identical model to task 1 but substituting `GRU` layers instead of <br>\n",
        "`RNN` layers and instead call the model `simplegru`. The model and training <br>\n",
        "should be otherwise identical to the training from above. Be sure to pass <br>\n",
        "simplegru's parameters to the optimizer and not simplernn's parameters.\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.GRU.html"
      ],
      "metadata": {
        "id": "gdmxr2joyzAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ],
      "metadata": {
        "id": "iIbbvXR7zopr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class SimpleGRU("
      ],
      "metadata": {
        "id": "1DTVPsUs6AsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simplegru = SimpleGRU(\n",
        "# criterion =\n",
        "# metric =\n",
        "# optimizer =\n",
        "# history, simplegru = train_and_validate("
      ],
      "metadata": {
        "id": "oEQ2_LBN_Gtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ],
      "metadata": {
        "id": "a89v3Z9izsgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(simplegru, test_loader, criterion, metric=metric, device=device)"
      ],
      "metadata": {
        "id": "AGknk6XItDz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X_new\n",
        "for step_ahead in range(10):\n",
        "    y_pred_one = simplegru(torch.from_numpy(X[:, step_ahead:]).to(device)).to('cpu').detach().numpy()[:, np.newaxis, :]\n",
        "    X = np.concatenate([X, y_pred_one], axis=1)\n",
        "\n",
        "y_pred_new = X[:, n_steps:]"
      ],
      "metadata": {
        "id": "N70bpTybf7GR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_multiple_forecasts(X_new, Y_new, y_pred_new)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BeMdII5jf_qC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Long Short-Term Memory (LSTM)\n",
        "\n",
        "LSTM's were among the state of the art for language processing tasks for quite <br>\n",
        "some time. Interestingly though, they were **first proposed in 1995** by Hochreiter <br>\n",
        "and Schmidhuber almost **20 years before GRU**. In fact, LSTM models **outperform** <br>\n",
        "**GRU models** on many tasks and by many metrics. However, **not without costs**. LSTM <br>\n",
        "cells require **more storage space** and are **slower** than GRU cells. This is due to <br>\n",
        "the fact that they have an additional **\"long-term memory\"** value which they can <br>\n",
        "use to preserve information even between iterations. Whereas GRU will only <br>\n",
        "\"remember\" information passed by adjacent neurons, **LSTM will remember** that and <br>\n",
        "also information passed by **previous batches or iterations of data**."
      ],
      "metadata": {
        "id": "gifx3hG1XQ6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Long Short-Term Memory (LSTM) Model\n",
        "\n",
        "Build an identical model to task 1 but substituting `LSTM` layers instead of <br>\n",
        "`RNN` layers and instead call the model `simplelstm`. The model and training <br>\n",
        "should be otherwise identical to the training from above. Be sure to pass <br>\n",
        "simplelstm's parameters to the optimizer and not simplernn's parameters.\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html"
      ],
      "metadata": {
        "id": "ybs2U22CL9le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class SimpleLSTM("
      ],
      "metadata": {
        "id": "iToQMqX5_EP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simplelstm = SimpleLSTM(\n",
        "# criterion =\n",
        "# metric =\n",
        "# optimizer =\n",
        "# history, simplelstm = train_and_validate("
      ],
      "metadata": {
        "id": "BNdOe0A9_EP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ],
      "metadata": {
        "id": "UFNMUz-y_EQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(simplelstm, test_loader, criterion, metric=metric, device=device)"
      ],
      "metadata": {
        "id": "bFVP8rGH_EQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X_new\n",
        "for step_ahead in range(10):\n",
        "    y_pred_one = simplelstm(torch.from_numpy(X[:, step_ahead:]).to(device)).to('cpu').detach().numpy()[:, np.newaxis, :]\n",
        "    X = np.concatenate([X, y_pred_one], axis=1)\n",
        "\n",
        "y_pred_new = X[:, n_steps:]"
      ],
      "metadata": {
        "id": "riJnXum6_EQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_multiple_forecasts(X_new, Y_new, y_pred_new)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fgJ0FKYe_EQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Analyzing Model Performance\n",
        "\n",
        "Question: Compare the behavior of LSTM to GRU and simple RNN. Which region of <br>\n",
        "future forecasting is most different? Describe characteristics about the <br>\n",
        "various models that would lead to that difference."
      ],
      "metadata": {
        "id": "TbkvrUiwz698"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ],
      "metadata": {
        "id": "8FQZsm8u0sR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3) answer:"
      ],
      "metadata": {
        "id": "l-mqOqLq0s7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ],
      "metadata": {
        "id": "DSyuEsHw0sR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Luong Attention Mechanism\n",
        "\n",
        "Attention is an operation built around applying a **dot product** across **1) outputs** <br>\n",
        "**from the head of a model** which has \"encoded\" the data by transforming with <br>\n",
        "weights and biases and **2) hidden states (weights) from the tail of a model** <br>\n",
        "which is attempting to \"decode\" the data and turn it into your desired output.\n",
        "\n",
        "That is a long sentence but what it breaks down to is that you're **reweighting** <br>\n",
        "**data** part way through a model so that the back half of your model knows what's <br>\n",
        "most important from what the first half did. When Luong, Pham and Manning <br>\n",
        "published [this paper](https://arxiv.org/abs/1508.04025) in 2015, they achieved\n",
        "**state of the art results** and laid the <br>\n",
        "foundation for the **Third Wave of AI** driven largely by attention-based <br>\n",
        "Transformer models."
      ],
      "metadata": {
        "id": "DoBM0jI8ZwrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, seq_len):\n",
        "        super(AttentionLSTM, self).__init__()\n",
        "        self.rnn1 = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.rnn2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size * seq_len, output_size)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "    def forward(self, x):\n",
        "        x, states = self.rnn1(x)\n",
        "        hn, cn = states\n",
        "        attention = self.softmax(hn)\n",
        "        context = torch.tensordot(attention, hn, dims=[[1,2],[1,2]])\n",
        "        hn = hn + context\n",
        "        x, _ = self.rnn2(x, (hn, cn))\n",
        "        x = x.reshape(-1, x.size(1) * x.size(2))\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "f8IudrApICJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attlstm = AttentionLSTM(1, 16, 1, 50)\n",
        "criterion = nn.MSELoss()\n",
        "metric = nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(attlstm.parameters(), lr=0.001)\n",
        "history, attlstm = train_and_validate(train_loader, valid_loader, attlstm, optimizer, criterion, num_epochs=6, metric=metric, scheduler=None, device=device)"
      ],
      "metadata": {
        "id": "cvZy0qGvJ_Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Understanding Luong Attention\n",
        "\n",
        "Here `hn` is the hidden state output from an encoder and `cn` is the cell <br>\n",
        "state from an encoder. Describe, to the best your ability, what is happening <br>\n",
        "between the extraction of the hidden state from the encoder and passing the <br>\n",
        "hidden state as the initial state of the decoder (between lines 2-6)."
      ],
      "metadata": {
        "id": "QAQaRKjY05D3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ],
      "metadata": {
        "id": "QeeQaY-g2nuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4) answer:"
      ],
      "metadata": {
        "id": "WquNeYjy2nuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ],
      "metadata": {
        "id": "uHVpLREr2nuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(attlstm, test_loader, criterion, metric=metric, device=device)"
      ],
      "metadata": {
        "id": "mwqxmR4dMDDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X_new\n",
        "for step_ahead in range(10):\n",
        "    y_pred_one = attlstm(torch.from_numpy(X[:, step_ahead:]).to(device)).to('cpu').detach().numpy()[:, np.newaxis, :]\n",
        "    X = np.concatenate([X, y_pred_one], axis=1)\n",
        "\n",
        "y_pred_new = X[:, n_steps:]"
      ],
      "metadata": {
        "id": "V3wvIiFqMDDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_multiple_forecasts(X_new, Y_new, y_pred_new)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ww_bHfIDMDDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Transformer Model\n",
        "\n",
        "The transformer architecture was first proposed in a paper called <br>\n",
        "[Attention is All You Need (Vaswani et. al 2017](https://arxiv.org/abs/1706.03762).\n",
        "\n",
        "The characteristic operation of the transformer is the **self-attention** <br>\n",
        "operation. In essence, it is using **dot product** operations along different axes <br>\n",
        "of the data to amplify the importance of relevant data. You can think of this <br>\n",
        "as effectively building **a model that reweights its inputs** to pick out the most <br>\n",
        "important and relevant inputs. Then it does typical mathematical operations of <br>\n",
        "**densely connected layers** on those reweighted inputs.\n",
        "\n",
        "Researchers figured out several years before this that dot-product attention <br>\n",
        "could be used in combination with other forms of models to improve performance <br>\n",
        "by reweighting input data but the **Attention is All You Need** paper showed that, <br>\n",
        "as the name would imply, you can get even **better performance** by eliminating all <br>\n",
        "other characteristic layers and instead **using only attention and dense layers**."
      ],
      "metadata": {
        "id": "mu6pVFsZWG89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)"
      ],
      "metadata": {
        "id": "BALMDUKmzW46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5: Definining model dimensions\n",
        "\n",
        "Define two variables: <br>\n",
        "1) `max_len` should be equal to the size of the dimension from X_train that <br>\n",
        "describes the length of the time series. **Hint: see above plots**<br>\n",
        "2) `embedding_dim` should be equal to the fourth root (**(1/4)) of the total <br>\n",
        "possible data points in the input and output. You can estimate this with the <br>\n",
        "number of unique sequences times sequence length."
      ],
      "metadata": {
        "id": "j1J2HsBJ2zwe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ],
      "metadata": {
        "id": "EHhIpWbF57L6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# max_len =\n",
        "# embedding_dim =\n",
        "# print(max_len)\n",
        "# print(embedding_dim)"
      ],
      "metadata": {
        "id": "iF2tRgjxdI8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ],
      "metadata": {
        "id": "m-Fphtin5wsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's typical with transformers to embed our inputs into a higher dimensional <br>\n",
        "space. For discrete data such as integers or words, it's typical to use an  <br>\n",
        "embedding layer from the keras toolkit. However, for continuous data, it's more <br>\n",
        "common to embed using linear or non-linear transformations."
      ],
      "metadata": {
        "id": "FnMI23J6lVGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearEmbedding(nn.Module):\n",
        "  def __init__(self, in_features, out_features, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.linear = nn.Linear(in_features=in_features, out_features=out_features)\n",
        "  def forward(self, x):\n",
        "    return self.linear(x)"
      ],
      "metadata": {
        "id": "uGMFrPX8QgST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another interesting feature of transformers is that they are not explicitly <br>\n",
        "aware of the order of data in a relative sequence, especially after embedding. <br>\n",
        "We therefore can inform it of positions by, for example, adding a small amount <br>\n",
        "related to its position in the tensor by some interpretable form like periodic <br>\n",
        "functions."
      ],
      "metadata": {
        "id": "QJHKxTVrlyWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len=512):\n",
        "        super(SinusoidalPositionalEncoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        #Note: The value of 10000.0 below corresponds to an encoding wavelength.\n",
        "        #You can often set a much smaller value when using lower-dimensional.\n",
        "        #The value of 10000.0 was designed for very high-dimensional data.\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, d_model = x.size()\n",
        "        pe = self.pe[:, :seq_len, :]\n",
        "        pe = pe.expand(batch_size, -1, -1)\n",
        "        x = x + pe / torch.norm(pe, p=2, dim=-1, keepdim=True)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ufzj9vPHVXRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer models use **multi-head attention (MHA)** which is very similar <br>\n",
        "to the attention we saw before with LSTM. Here, we treat our data as having <br>\n",
        "the three dimensions **(batch_size, sequence_length, features)**. We use the <br>\n",
        "linear embedding layer we saw before to **add additional information** to the <br>\n",
        "features dimension in our data. In transformer notation, typically data is <br>\n",
        "expressed as having the dimensions **(q,k,v)** or **(query, key value)**. That means <br>\n",
        "each item in the **batch is a query** that try get get information for. We use <br>\n",
        "the dot product operation for that query item with the other **keys and values** <br>\n",
        "or **sequences and features** in the data and then add that information back to <br>\n",
        "the query and then normalize to avoid exploding values.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Short summary:**<br>\n",
        "What this means is that at their core, transformers use dot product attention <br>\n",
        "to **extract additional information** about how items in the sequence are **related** <br>\n",
        "to one another, **add that information** to the original data, and then pass <br>\n",
        "the modified data to a dense neural network."
      ],
      "metadata": {
        "id": "aG18vFg8-6w-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NextTokenPredictionTransformer(nn.Module):\n",
        "        def __init__(self, embed_dim, latent_dim, num_heads, num_encoders, num_decoders, dropout, sequence_length, device):\n",
        "            super(NextTokenPredictionTransformer, self).__init__()\n",
        "            self.seq_len = sequence_length\n",
        "            self.trans = nn.Transformer(d_model=embed_dim, nhead=num_heads,\n",
        "                                        num_encoder_layers=num_encoders, num_decoder_layers=num_decoders,\n",
        "                                        dim_feedforward=latent_dim, dropout=dropout, batch_first=True)\n",
        "            self.sinemb = SinusoidalPositionalEncoding(embed_dim, sequence_length)\n",
        "            self.linemb = LinearEmbedding(1, embed_dim)\n",
        "            self.lin1 = nn.Linear(embed_dim, latent_dim)\n",
        "            self.relu = nn.ReLU()\n",
        "            self.lin2 = nn.Linear(latent_dim, 1)\n",
        "\n",
        "        def forward(self, src, tgt):\n",
        "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = self.create_mask(src, tgt)\n",
        "            src_padding_mask = src_padding_mask.squeeze(-1)\n",
        "            tgt_padding_mask = tgt_padding_mask.squeeze(-1)\n",
        "            src = self.linemb(src)\n",
        "            tgt = self.linemb(tgt)\n",
        "            src = self.sinemb(src)\n",
        "            tgt = self.sinemb(tgt)\n",
        "            out = self.trans(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask,\n",
        "                             src_key_padding_mask=src_padding_mask,\n",
        "                             tgt_key_padding_mask=tgt_padding_mask)\n",
        "            output = self.lin1(out)\n",
        "            output = self.relu(output)\n",
        "            output = self.lin2(output)\n",
        "            return output\n",
        "\n",
        "\n",
        "        def generate_square_subsequent_mask(self, sz, device):\n",
        "            mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
        "            mask = ~mask\n",
        "            return mask\n",
        "\n",
        "        def create_mask(self, src, tgt):\n",
        "            src_seq_len = src.shape[1]\n",
        "            tgt_seq_len = tgt.shape[1]\n",
        "\n",
        "            tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len, device)\n",
        "            src_mask = torch.zeros((src_seq_len, src_seq_len),device=device).type(torch.bool)\n",
        "\n",
        "            src_padding_mask = (src == 0)\n",
        "            tgt_padding_mask = (tgt == 0)\n",
        "            return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "metadata": {
        "id": "4WFVL_JG9uc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: Transformers are often much larger in dimensionality than RNNs so we may** <br>\n",
        "**need to use different training procedure like schedules for best results.**"
      ],
      "metadata": {
        "id": "qPz2P5Ab6LvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_train_and_validate(train_loader, val_loader, model, optimizer, criterion, num_epochs, metric=None, scheduler=None, device='cpu'):\n",
        "    history = {\n",
        "        'epoch': [],\n",
        "        'train_loss': [],\n",
        "        'train_metric': [],\n",
        "        'val_loss': [],\n",
        "        'val_metric': [],\n",
        "        'learning_rate': []\n",
        "    }  # Initialize a dictionary to store epoch-wise results\n",
        "\n",
        "    model.to(device)  # Move the model to the specified device\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        epoch_loss = 0.0  # Initialize the epoch loss and metric values\n",
        "        epoch_metric = 0.0\n",
        "\n",
        "        # Training loop\n",
        "        for X, y in train_loader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device).unsqueeze(-1)\n",
        "            optimizer.zero_grad()  # Clear existing gradients\n",
        "            y_input = torch.cat((X[:, 1:], y[:, -1:]), dim=1)\n",
        "            outputs = model(X[:,1:], y_input[:,:-1])\n",
        "            # Reshape outputs and targets to match the shape expected by the loss function\n",
        "            outputs = outputs.view(-1)\n",
        "            y_input = y_input[:,1:].contiguous().view(-1)\n",
        "            loss = criterion(outputs, y_input)  # Compute the loss\n",
        "            loss.backward()  # Compute gradients\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # THESE LINES HAVE BEEN UPDATED TO ACCOUNT FOR DEFAULT ARGUMENTS\n",
        "            if metric is not None:\n",
        "                epoch_metric += metric(outputs, y_input)\n",
        "            else:\n",
        "                epoch_metric += 0.0\n",
        "\n",
        "        # Average training loss and metric\n",
        "        epoch_loss /= len(train_loader)\n",
        "        epoch_metric /= len(train_loader)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        with torch.no_grad():  # Disable gradient calculation\n",
        "            val_loss = 0.0\n",
        "            val_metric = 0.0\n",
        "            for X_val, y_val in val_loader:\n",
        "                X_val = X_val.to(device)\n",
        "                y_val = y_val.to(device).unsqueeze(-1)\n",
        "                y_input = torch.cat((X_val[:, 1:], y_val[:, -1:]), dim=1)\n",
        "                outputs_val = model(X_val[:, 1:], y_input[:,:-1])\n",
        "                outputs_val = outputs_val.view(-1)\n",
        "                y_input = y_input[:,1:].contiguous().view(-1)\n",
        "                val_loss += criterion(outputs_val, y_input).item()\n",
        "                if metric is not None:\n",
        "                    val_metric += metric(outputs_val, y_input).item()\n",
        "                else:\n",
        "                    val_metric += 0.0\n",
        "\n",
        "            val_loss /= len(val_loader)\n",
        "            val_metric /= len(val_loader)\n",
        "\n",
        "        # Append epoch results to history\n",
        "        history['epoch'].append(epoch)\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "        history['train_metric'].append(epoch_metric)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_metric'].append(val_metric)\n",
        "        history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, '\n",
        "              f'Train Metric: {epoch_metric:.4f}, Val Loss: {val_loss:.4f}, '\n",
        "              f'Val Metric: {val_metric:.4f}')\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "    return history, model"
      ],
      "metadata": {
        "id": "tcriE5V4dSsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 6: Build a train a NextTokenPredictionTransformer\n",
        "\n",
        "The transformer parameters should be:<br>\n",
        "`embed_dim` = embedding_dim<br>\n",
        "`latent_dim` = 16<br>\n",
        "`num_heads` = 1<br>\n",
        "`num_encoders` = 1<br>\n",
        "`num_decoders` = 1<br>\n",
        "`dropout` = 0.0<br>\n",
        "`sequence_length` = max_len<br>\n",
        "\n",
        "Transformers are a bit less stable to train so we'll need to use a slightly <br>\n",
        "more complicated training procedure.\n",
        "\n",
        "Use Adam optimizer with `lr=0.01` and train for 6 epochs using MSELoss as <br>\n",
        "your criterion and L1Loss as your metric. Afterward, reinitialize the Adam <br>\n",
        "optimizer with `lr=0.001` and train for an additional 6 epochs to fine tune.\n"
      ],
      "metadata": {
        "id": "9NF3JVu6CIbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.manual_seed(42)\n",
        "# transformer = NextTokenPredictionTransformer(\n",
        "# criterion =\n",
        "# metric =\n",
        "# optimizer =\n",
        "# history, transformer = transformer_train_and_validate(\n",
        "# optimizer =\n",
        "# history, transformer = transformer_train_and_validate("
      ],
      "metadata": {
        "id": "-cBRAmg4Vme5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_next_step(model, input_sequence):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_sequence = input_sequence.to(device)\n",
        "        output = model(input_sequence[:, 1:], input_sequence[:, :-1])\n",
        "        next_token_prediction = output[:, -1, :]\n",
        "    return next_token_prediction"
      ],
      "metadata": {
        "id": "2af1FE6QCjji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X_new\n",
        "for step_ahead in range(10):\n",
        "    y_pred_one = generate_next_step(transformer, torch.from_numpy(X[:, step_ahead:]).to(device)).to('cpu').detach().numpy()[:, np.newaxis, :]\n",
        "    X = np.concatenate([X, y_pred_one], axis=1)\n",
        "\n",
        "y_pred_new = X[:, n_steps:]"
      ],
      "metadata": {
        "id": "LJ3zPYLAlsaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_multiple_forecasts(X_new, Y_new, y_pred_new)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pTLGchPClsaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChatGPT\n",
        "\n",
        "GPT stands for **\"generative, pre-trained transformer\"**. ChatGPT is a version of <br>\n",
        "a gpt **created by OpenAI** which made waves in late 2022 because of how well it <br>\n",
        "mimics human conversation and interaction. Below, we'll look at how to access <br>\n",
        "**ChatGPT in colab using a public API**. It is not possible to house the full model <br>\n",
        "in a typical colab notebook because the model has an astounding **175 billion** <br>\n",
        "**model parameters**. For comparison, two fully connected dense layers with 10,000 <br>\n",
        "neurons each would only have around 100 million parameters.\n",
        "\n",
        "In March 2023, OpenAI also released GPT4 which has over **1 TRILLION!** <br>\n",
        "parameters. The model extended the capabilities of previous GPT iterations to <br>\n",
        "include vision. It's so far shown incredible ability, even able to score in the <br>\n",
        "**90th percentile on the Bar exam**, the final examination to become a lawyer in <br>\n",
        "the United States."
      ],
      "metadata": {
        "id": "h823RwThqvCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "15dacVG8mmXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Create an acccount at https://platform.openai.com\n",
        "# Replace YOUR_API_KEY with your actual API key for the ChatGPT service:\n",
        "# https://platform.openai.com/account/api-keys\n",
        "my_api_key = \"YOUR_API_KEY\""
      ],
      "metadata": {
        "id": "wBNp6dPKmoFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = openai.OpenAI(api_key=my_api_key)\n",
        "response = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is dot-product attention?\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-3.5-turbo\", # The specific GPT model you want to use\n",
        "    max_tokens=300, # Max number of characters in output\n",
        "    n=1, # Number of responses to generate\n",
        "    stop=None, # When the output stops\n",
        "    temperature=0.0, # Controls the randomness of the output (>=0)\n",
        ")\n",
        "\n",
        "message = response.choices[0].text.strip()\n",
        "print(message)"
      ],
      "metadata": {
        "id": "0SrJomqfmu7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 7: ChatGPT for Education\n",
        "\n",
        "Using ChatGPT, generate an interesting piece of educational content about the <br>\n",
        "current topic of RNNs and/or Transformers. **Hint: better promt=better response**<br>\n",
        "\n",
        "**IF YOU EXCEED YOUR FREE OPENAI QUOTA JUST RUN THE CODE TO GENERATE A** <br>\n",
        "**RESPONSE AND LEAVE THE ERROR MESSAGE IN THE OUTPUT CELL**"
      ],
      "metadata": {
        "id": "QPopArby65S2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ],
      "metadata": {
        "id": "aZ0Xck2m65S3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sis88Wfw7X8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ],
      "metadata": {
        "id": "OMnhBJ6w65S3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 8 (Bonus): Model performance analysis\n",
        "\n",
        "Analyze the performance of at least 3 models used in this exercise using at <br>\n",
        "least two different metrics on the test data sets or newly generated data. <br>\n",
        "Examples of metrics might be single-event prediction error, forecasting error, <br>\n",
        "or deviation-based metrics.\n",
        "\n",
        "Further, provide a brief text description (less than 500 words) explaining the <br>\n",
        "metric used and results."
      ],
      "metadata": {
        "id": "bDTuLad08Bzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ],
      "metadata": {
        "id": "iT2iabKo8Bzo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Xpb1p238Bzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 8) answer:"
      ],
      "metadata": {
        "id": "dpf6qZRQ-TTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ],
      "metadata": {
        "id": "biRtG7Tr8Bzo"
      }
    }
  ]
}