{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UAPH451551/PH451_551_Sp24/blob/main/Exercises/05_neural_nets_with_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUOwclhJHNDP"
      },
      "source": [
        "# Hands On #5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piBSRAqc96nN"
      },
      "source": [
        "**Chapter 10 – Introduction to Artificial Neural Networks with Pytorch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFOtkrf3HNDS"
      },
      "source": [
        "File name convention: For group 42 and members Richard Stallman and Linus <br> Torvalds it would be: <br>\n",
        "\"05_neural_nets_with_pytorch_Stallman_Torvalds.pdf\"\n",
        "\n",
        "Submission via blackboard (UA).\n",
        "\n",
        "Feel free to answer free text questions in text cells using markdown and <br>\n",
        "possibly $\\LaTeX{}$ if you want to.\n",
        "\n",
        "**You don't have to understand every line of code here and it is not intended <br>\n",
        "for you to try to understand every line of code.<br>\n",
        "Big blocks of code are usually meant to just be clicked through.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3TTeAuN96nU"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3_Ulmke96nV"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "import torch\n",
        "assert torch.__version__ >= \"2.0\"\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKK8dS-C96nW"
      },
      "source": [
        "# Perceptrons\n",
        "\n",
        "**Perceptrons are a form of linear classifier**. The characteristic expression is <br>\n",
        "$\\Sigma_i w_i \\cdot x_i + b$. Where x are your inputs and w are your model\n",
        "weights<br>\n",
        "and b is a learnable bias term. The classification part comes in by setting <br>\n",
        "**any positive result of the above expression as the 1 or True label** and any <br> **negative result as the 0 or False label**. We then use stochastic gradient <br>\n",
        "descent to optimize the weights and bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc419YFJ96nX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data[:, (2, 3)]  # petal length, petal width\n",
        "y = (iris.target == 0).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tut21LOA-I2r"
      },
      "source": [
        "### Task 1:\n",
        "Fit the iris dataset into a Perceptron layer and predict the class of a sample <br>\n",
        "with petal length of 2 and a petal width of 0.5.\n",
        "\n",
        "Use:\n",
        "`max_iter=1000`, `tol =1e-3` and `random_state= 42`.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2GbE_Wj96nW"
      },
      "source": [
        "**Note**: we set `max_iter` and `tol` explicitly to avoid warnings about the <br>\n",
        "fact that their default value will change in future versions of Scikit-Learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsnymzLuLvm2"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5wg3vIj3gxL"
      },
      "outputs": [],
      "source": [
        "#per_clf ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU8nJGQo96nY"
      },
      "outputs": [],
      "source": [
        "#y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L33GBuxILvm4"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzqgH5uG96nZ"
      },
      "outputs": [],
      "source": [
        "a = -per_clf.coef_[0][0] / per_clf.coef_[0][1]\n",
        "b = -per_clf.intercept_ / per_clf.coef_[0][1]\n",
        "\n",
        "axes = [0, 5, 0, 2]\n",
        "\n",
        "x0, x1 = np.meshgrid(\n",
        "        np.linspace(axes[0], axes[1], 500).reshape(-1, 1),\n",
        "        np.linspace(axes[2], axes[3], 200).reshape(-1, 1),\n",
        "    )\n",
        "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
        "y_predict = per_clf.predict(X_new)\n",
        "zz = y_predict.reshape(x0.shape)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\", label=\"Not Iris-Setosa\")\n",
        "plt.plot(X[y==1, 0], X[y==1, 1], \"yo\", label=\"Iris-Setosa\")\n",
        "\n",
        "plt.plot([axes[0], axes[1]], [a * axes[0] + b, a * axes[1] + b], \"k-\", linewidth=3)\n",
        "from matplotlib.colors import ListedColormap\n",
        "custom_cmap = ListedColormap(['#9898ff', '#fafab0'])\n",
        "\n",
        "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
        "plt.xlabel(\"Petal length\", fontsize=14)\n",
        "plt.ylabel(\"Petal width\", fontsize=14)\n",
        "plt.legend(loc=\"lower right\", fontsize=14)\n",
        "plt.axis(axes)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7AzNDaU-xOV"
      },
      "source": [
        "### Task 2\n",
        " Elaborate on the difference between a perceptron and logistic regression.\n",
        "\n",
        " **Hint:** Consider the nature of the boundary in the above plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW0MVBaoHNDW"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k64m9OUeHNDW"
      },
      "source": [
        "Task 2 answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdoyypmcHNDW"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZr175SP96na"
      },
      "source": [
        "# Activation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBhSPTlaDZcE"
      },
      "source": [
        "### Task 3\n",
        "Describe the role of activation functions within a neural network. If you build <br>\n",
        "a neural network with no activation function, which model that we've seen in <br>\n",
        "this class would your network resemble?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9pD5KasHNDW"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSarFW4JHNDX"
      },
      "source": [
        "Task 3 answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rETgKA70HNDX"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zZHVAV096nd"
      },
      "source": [
        "# Building an Image Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6d_vhWv96nd"
      },
      "source": [
        "First let's import Pytorch. **Pytorch is a machine learning platform developed** <br>\n",
        "**by Meta**. It is now a free, open-source platform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSUOusSb96nd"
      },
      "outputs": [],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IC001Fj96ne"
      },
      "source": [
        "Let's start by loading the [fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset. Keras has a number <br>\n",
        "of functions to load popular datasets in `keras.datasets`. **The dataset is** <br> **already split for you between a training set and a test set**, but it can be <br> useful to split the training set further to have a validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpvihLLAiCtJ"
      },
      "source": [
        "Keras is another machine learning toolkit that acts as the python interface <br>\n",
        "for tensorflow. We generally won't use it in this course other than for <br>\n",
        "accessing some data collections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2XiMCMx96nd"
      },
      "outputs": [],
      "source": [
        "import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rR7u1KhvlU5E"
      },
      "outputs": [],
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm2mJI2v96ne"
      },
      "source": [
        "The training set contains 60,000 grayscale images, each 28x28 pixels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojm1jhI7ybiU"
      },
      "outputs": [],
      "source": [
        "X_train_full.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q7eH1GS96nf"
      },
      "source": [
        "Each pixel intensity is represented as a byte (0 to 255):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdlXIZMO-3Cn"
      },
      "outputs": [],
      "source": [
        "X_train_full.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up_jreEp96ng"
      },
      "source": [
        "Let's **split the full training set into a validation set and a (smaller)** <br> **training set**. We also **scale the pixel intensities down to the 0-1 range** and <br>\n",
        "convert them to floats, by dividing by 255. This is essentially min-max scaling <br>\n",
        "or normalization for pixels with a maximum value of 255 and a minimum of 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAZzoYRD96ng"
      },
      "outputs": [],
      "source": [
        "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "X_test = X_test / 255."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG-ZXYbM96ng"
      },
      "source": [
        "You can plot an image using Matplotlib's `imshow()` function, with a `'binary'` <br>\n",
        "color map:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5qfRYzl96ng"
      },
      "outputs": [],
      "source": [
        "plt.imshow(X_train[0], cmap=\"binary\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HIhKfBK96nh"
      },
      "source": [
        "The labels are the class IDs (represented as uint8), from 0 to 9:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CC0hpimX96nh"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuQH9NeY96nh"
      },
      "source": [
        "Here are the corresponding class names:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DnDTta596nh"
      },
      "outputs": [],
      "source": [
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z3ctcPE96ni"
      },
      "source": [
        "So the first image in the training set is a coat:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wo3nXTyr96ni"
      },
      "outputs": [],
      "source": [
        "class_names[y_train[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSVf64Kv96ni"
      },
      "source": [
        "The validation set contains 5,000 images, and the test set contains 10,000 images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3DPLv7496ni"
      },
      "outputs": [],
      "source": [
        "X_valid.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBVt2-n196nj"
      },
      "outputs": [],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr7f-I2i96nj"
      },
      "source": [
        "Let's take a look at a sample of the images in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiwEaciG96nj"
      },
      "outputs": [],
      "source": [
        "n_rows = 4\n",
        "n_cols = 10\n",
        "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        index = n_cols * row + col\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
        "        plt.axis('off')\n",
        "        plt.title(class_names[y_train[index]], fontsize=12)\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0XW9zPwEs9r"
      },
      "source": [
        "**Data Loaders:**\n",
        "Pytorch is built on a data type called a tensor. Numpy arrays are not <br>\n",
        "themselves tensors. So we'll use a *dataloader* to **convert our numpy arrays to** <br>\n",
        "**tensors** and pass those to our pytorch model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hea2aFOuFayd"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRIUnQUAEsNW"
      },
      "outputs": [],
      "source": [
        "class MnistDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X.copy()).float()\n",
        "        self.y = torch.from_numpy(y.copy()).long()\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tp-feDfY-14P"
      },
      "outputs": [],
      "source": [
        "train_data = MnistDataset(X_train, y_train)\n",
        "valid_data = MnistDataset(X_valid, y_valid)\n",
        "test_data = MnistDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
        "valid_loader = DataLoader(valid_data, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an [introductory tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) to using Datasets and Dataloaders in Pytorch that <br> you may find helpful."
      ],
      "metadata": {
        "id": "CMpzLBpwgobd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1G5vYM4AEYy"
      },
      "source": [
        "## Defining a neural network using Pytorch:\n",
        "\n",
        "In the cells below we  build an deep neural network model using the Pytorch <Br>\n",
        "Sequential class. The input is an image of shape 28 by 28 whose dimensions are <br> reshaped to a flat 1d array of length $28^2=784$ by the `nn.Flatten` layer. <br>\n",
        "The network consists of `Linear` layers (fully-connected multi-output <br> perceptrons) sandwiched between non-linear activations `ReLU` and `Softmax`.<br> The `Softmax` output activation function is used for multi-label classification. <br>\n",
        "The layers are wrapped in a `nn.Sequential` function that executes each <br> operation in the given order when the `model` object is called. <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFYpKULg96nk"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHTrilwL96nk"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(28*28, 300),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(300, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100, 10),\n",
        "    nn.Softmax(dim=1)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This is essentially a multi-layer perceptron model with activation functions** <br>\n",
        "to each \"neuron\". Additionally, **we no longer end the model with a decision** <br>\n",
        "**function** that outputs either a 0 or 1. Here our final layer's **weights** <br>\n",
        "**will be passed through a softmax function which will output a** <br>\n",
        "**collection of 10 values which add up to 1**. These are the probabilities of each <br>\n",
        "class being the correct class."
      ],
      "metadata": {
        "id": "05UJj6PrlMY9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6wLCkUc96nk"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFvLmimPHNDc"
      },
      "source": [
        "We can look at the layers directly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3skG5ULDHNDc"
      },
      "outputs": [],
      "source": [
        "hidden1 = model[1] # 1st hidden layer is at index 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eR5m6LjX96nl"
      },
      "outputs": [],
      "source": [
        "weights, biases = hidden1.weight, hidden1.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETO_Mi2f96nm"
      },
      "outputs": [],
      "source": [
        "weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-AWFPWX96nm"
      },
      "outputs": [],
      "source": [
        "weights.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qb7RJWWg96nm"
      },
      "outputs": [],
      "source": [
        "biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRfITXCA96nm"
      },
      "outputs": [],
      "source": [
        "biases.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDu2dLD8BbeJ"
      },
      "source": [
        "## Training loop in Pytroch\n",
        "In pytorch we need to **create loops** that do things like **training** our model. <br>\n",
        "Below is an example training loop that we'll use in this code. Each step is <br>\n",
        "commented and it's **very important to understand this code** in order to use <br>\n",
        "pytorch and other similar machine learning libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNjZ9XsLEloH"
      },
      "outputs": [],
      "source": [
        "def train_and_validate(train_loader, val_loader, model, optimizer, criterion, num_epochs, metric=None):\n",
        "    history = {\n",
        "        'epoch': [],\n",
        "        'train_loss': [],\n",
        "        'train_metric': [],\n",
        "        'val_loss': [],\n",
        "        'val_metric': []\n",
        "    }  # Initialize a dictionary to store epoch-wise results\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        epoch_loss = 0.0  # Initialize the epoch loss and metric values\n",
        "        epoch_metric = 0.0\n",
        "\n",
        "        # Training loop\n",
        "        for X, y in train_loader:\n",
        "            optimizer.zero_grad()  # Clear existing gradients\n",
        "            outputs = model(X)  # Make predictions\n",
        "            loss = criterion(outputs, y)  # Compute the loss\n",
        "            loss.backward()  # Compute gradients\n",
        "            optimizer.step()  # Update model parameters\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            if metric is not None:\n",
        "                epoch_metric += metric(outputs, y)\n",
        "            else:\n",
        "                epoch_metric += 0\n",
        "\n",
        "        # Average training loss and metric\n",
        "        epoch_loss /= len(train_loader)\n",
        "        epoch_metric /= len(train_loader)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        with torch.no_grad():  # Disable gradient calculation\n",
        "            val_loss = 0.0\n",
        "            val_metric = 0.0\n",
        "            for X_val, y_val in val_loader:\n",
        "                outputs_val = model(X_val)  # Make predictions\n",
        "                val_loss += criterion(outputs_val, y_val).item()  # Compute loss\n",
        "                if metric is not None:\n",
        "                    val_metric += metric(outputs_val, y_val)\n",
        "                else:\n",
        "                    val_metric += 0\n",
        "\n",
        "            val_loss /= len(val_loader)\n",
        "            val_metric /= len(val_loader)\n",
        "\n",
        "        # Append epoch results to history\n",
        "        history['epoch'].append(epoch_loss)\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "        history['train_metric'].append(epoch_metric)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_metric'].append(val_metric)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, '\n",
        "              f'Train Metric: {epoch_metric:.4f}, Val Loss: {val_loss:.4f}, '\n",
        "              f'Val Metric: {val_metric:.4f}')\n",
        "\n",
        "    return history, model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB7Fl7Bprtzs"
      },
      "source": [
        "**Model parameters:**\n",
        "Here, we choose the **\"sgd\" optimizer**. The SGD optimizer is a derivative of <br>\n",
        "the SGD algorithm but, instead of updating coefficients for a linear <br> regression as we saw in previous exercises, we compute the **gradient of our** <br>\n",
        "**loss function** (sparse categorical crossentropy) **with respect to our model** <br>\n",
        "**weights and layer biases** and use that to update our weights and biases. Note <br>\n",
        "that our gradient will now be the sum of more complicated partial <br>\n",
        "derivatives. Note that **our gradient will now be the sum of more complicated** <br>\n",
        "**partial derivatives**. $\\partial L/\\partial w_i = (\\partial L / \\partial f(w_i)) (\\partial f(w_i) / \\partial w_i)$ where <br>\n",
        "$f(w_i) = ReLU(w_ix_i + b)$. Additionally we may have a partial derivative for <br>\n",
        "our bias term $\\partial L/\\partial b$. The total gradient for stochastic gradient descent will then be <br>\n",
        "$\\nabla L_{layer} = \\Sigma_i \\partial L/\\partial w_i + \\partial L/\\partial b$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iha1BNF296nn"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "def accuracy_metric(pred, target):\n",
        "    if pred.shape[1] > 1:\n",
        "        pred = pred.argmax(dim=1)\n",
        "        accuracy = torch.sum(pred == target).item() / len(pred)\n",
        "    else:\n",
        "        accuracy = torch.sum(pred > 0.5 == target).item() / len(pred)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z01D2HtY6B4r"
      },
      "source": [
        "Now let's train the model for 30 epochs.\n",
        "\n",
        "We save the most crucial parameters (`['loss', 'accuracy', 'val_loss',` <br> '`val_accuracy']`) in a dictionary named \"history\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niNnbD7U96no"
      },
      "outputs": [],
      "source": [
        "history, model = train_and_validate(train_loader, valid_loader, model,\n",
        "                                    optimizer=optimizer, criterion=criterion,\n",
        "                                    num_epochs=30, metric=accuracy_metric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F73axjgFLkI_"
      },
      "outputs": [],
      "source": [
        "print(history.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdaUfmYb96np"
      },
      "outputs": [],
      "source": [
        "print(history['epoch'])\n",
        "print(history['train_loss'])\n",
        "print(history['train_metric'])\n",
        "print(history['val_loss'])\n",
        "print(history['val_metric'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ht9uxLpwhpcR"
      },
      "source": [
        "Let's visualize the learning curves for this model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCv_GPMi96np"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "losses = pd.DataFrame({\n",
        "    'Train Loss': history['train_loss'],\n",
        "    'Validation Loss': history['val_loss']\n",
        "})\n",
        "\n",
        "# Plotting\n",
        "losses.plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.title(\"Training and Validation Losses\")\n",
        "plt.gca().set_ylim(1.5, 2.5)  # Adjust the y-axis limits if needed\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24ys_9GoNaiO"
      },
      "outputs": [],
      "source": [
        "accuracies = pd.DataFrame({\n",
        "    'Train Accuracy': history['train_metric'],\n",
        "    'Validation Accuracy': history['val_metric']\n",
        "})\n",
        "\n",
        "# Plotting\n",
        "accuracies.plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.title(\"Training and Validation Accuracies\")\n",
        "plt.gca().set_ylim(0, 1)  # Adjust the y-axis limits if needed\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfayrKJ8EVEi"
      },
      "source": [
        "## Testing loop\n",
        "We also need to create a testing loop if we want to check the performance of <br>\n",
        "our model after we train it. Notice a few key things:\n",
        "1. We put the model into evaluation mode instead of training mode\n",
        "2. We run the model without computing any gradients\n",
        "3. There is no more optimizer needed\n",
        "\n",
        "We also did this with the validation portion of `train_and_validate` loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXKJG4XMEUOv"
      },
      "outputs": [],
      "source": [
        "def test_model(data_loader, criterion, metric):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    total_loss = 0.0  # Initialize the total loss and metric values\n",
        "    total_metric = 0.0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient tracking\n",
        "        for batch in data_loader:\n",
        "            X, y = batch\n",
        "\n",
        "            # Pass the data to the model and make predictions\n",
        "            outputs = model(X)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "            # Add the loss and metric for the batch to the total values\n",
        "            total_loss += loss.item()\n",
        "            total_metric += metric(outputs, y)\n",
        "\n",
        "    # Average loss and metric for the entire dataset\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    avg_metric = total_metric / len(data_loader)\n",
        "\n",
        "    print(f'Test Loss: {avg_loss:.4f}, Test Metric: {avg_metric:.4f}')\n",
        "\n",
        "    return avg_loss, avg_metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2WdGqAlC0bm"
      },
      "source": [
        "### Task 4\n",
        " Validate your model using `test_model` using the test dataloader `test_loader`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYn6hY0XHNDf"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfC_sLA_96nq"
      },
      "outputs": [],
      "source": [
        "# evaluate model on  test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPp1YYCOHNDg"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM8ssoN0FjYS"
      },
      "source": [
        "### Task 5\n",
        "Select the first three samples from the test dataset not the dataloader and\n",
        "<br>\n",
        "predict their corresponding classes using: <br>\n",
        "`prediction = model(torch.from_numpy(X_new).float())`. Then print the <br>\n",
        "names/ categories of the elements in  question (Eg. \"Pants\", \"trouser\")\n",
        "\n",
        "You'll need to use `prediction.detach().numpy()` to get the outputs in a nice <br>\n",
        "numpy format.\n",
        "\n",
        "Also, use `np.argmax(prediction, axis=-1)` on the one-hot-encoded predicitons <br>\n",
        "to get the classes numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C0en5Jk96nq"
      },
      "outputs": [],
      "source": [
        "X_new = X_test[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMl_pVJMHNDg"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WoScu8CHNDg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arO65xXFHNDg"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNkCYEEIHNDg"
      },
      "outputs": [],
      "source": [
        "y_test[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MloueY3o96nr"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7.2, 2.4))\n",
        "for index, image in enumerate(X_new):\n",
        "    plt.subplot(1, 3, index + 1)\n",
        "    plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\")\n",
        "    plt.axis('off')\n",
        "    plt.title(class_names[y_test[index]], fontsize=12)\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdQPEnaa96nr"
      },
      "source": [
        "# Regression MLP\n",
        "\n",
        "We can also build multi-layer perceptrons for regression. The difference here <br>\n",
        "is that we remove features like softmax that are specific to multi-class <br>\n",
        "classification and instead end with an appropriately sized linear layer and <br>\n",
        "use mean-squared-error as our loss function instead of cross-entropy loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMxzRNLS96ns"
      },
      "source": [
        "Let's load, split and scale the [California housing dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJ3bFpfU96ns"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJiLPiB5Gzht"
      },
      "source": [
        "### Task 6\n",
        " Scale your training, validation, and test feature matrices using scikit-learn's <br>\n",
        "[StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). Best practice is to fit your scaler to either the full dataset <br>\n",
        "or a large subset e.g. X_train and scale all samples using the same means and <br>\n",
        "standard deviations. It is critical that you **use the same mean and standard**<br>\n",
        "**deviation for all data**. This standard scaler is implementing <br> standardization also sometimes referred to as standard normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWwom68-HNDh"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0ehUJYh7Dcw"
      },
      "outputs": [],
      "source": [
        "#scaler =\n",
        "#scaler = scaler.fit(X_train)\n",
        "#X_train =\n",
        "#X_valid =\n",
        "#X_test ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z4Q-GaqHNDh"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Task 7\n",
        "\n",
        "Create a dataloader for the [California housing dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html).\n",
        "\n",
        "Be sure to use appropriate data types for the inputs (X) and outputs (y). You must <br>\n",
        "include the functions `__init__`, `__len__`, and `__getitem__`. Then, create <br>\n",
        "your data sets and data loaders."
      ],
      "metadata": {
        "id": "Yyn_HypzGEBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class CaliHousingDataset(Dataset):\n",
        "#     def __init__(\n",
        "#     def __len__(\n",
        "#     def __getitem__("
      ],
      "metadata": {
        "id": "Guun0gb3GDJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data = CaliHousingDataset(\n",
        "# test_data = CaliHousingDataset(\n",
        "# valid_data = CaliHousingDataset(\n",
        "# train_loader = DataLoader(train_data\n",
        "# test_loader = DataLoader(test_data\n",
        "# valid_loader = DataLoader(valid_data"
      ],
      "metadata": {
        "id": "-cQZeWScL6VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv60cUzR96ns"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NgYV0XyIlfr"
      },
      "source": [
        "### Task 8\n",
        "This is a Neural Network with one hidden layer with 30 neurons. The output\n",
        "<br>\n",
        "layer has one neuron, which is the regression value. **Create a** <br> **train_and_validate loop for this model. Build the model, and train it.**\n",
        "\n",
        "This task is similar to task 5 except that we now do **regression, NOT** <br> **classification**. There will be some changes like no longer passing an accuracy <br>\n",
        "metric since that's not a regression metric. We'll also remove the softmax <br>\n",
        "which is used for multi-class classification. Finally, we'll need to use an <br>\n",
        "[appropriate loss function](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html).\n",
        "\n",
        "Read through the [California housing dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html) documentation to determine the <br>\n",
        "correct number of features and classes for the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M37QSQ8jHNDi"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHxGkzxAheie"
      },
      "outputs": [],
      "source": [
        "# # Note: No flatten layer or softmax layer and last dimension should be 1\n",
        "# model = nn.Sequential(\n",
        "#     #Linear layer that connects input features to 30 neurons\n",
        "#     #ReLU activation function\n",
        "#     #Linear layer that connects 30 hidden dims to classes\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj5v_bRU96ns"
      },
      "outputs": [],
      "source": [
        "# # Note: No metric needed and remember to change the loss function\n",
        "# def train_and_validate(train_loader, val_loader, model, optimizer,\n",
        "#                       criterion, num_epochs, metric=None):\n",
        "\n",
        "\n",
        "# ...\n",
        "# history, model ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoSs90vjHNDj"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5G8ujXkmHNDj"
      },
      "outputs": [],
      "source": [
        "losses = pd.DataFrame({\n",
        "    'Train Loss': history['train_loss'],\n",
        "    'Validation Loss': history['val_loss']\n",
        "})\n",
        "\n",
        "# Plotting\n",
        "losses.plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.title(\"Training and Validation Losses\")\n",
        "plt.gca().set_ylim(1.5, 2.5)  # Adjust the y-axis limits if needed\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSADjQQDJLgh"
      },
      "source": [
        "### Task 9.1\n",
        "Create a `test_model` function and evalute your model's performance by using <br>\n",
        "it on the test set. Also predict one element of the test set of your choice <br>\n",
        "(`X_test[42]` for example) and compare to the real value. You can look at the <br>\n",
        "validation part of the `test_model` function from before as a reference for <br>\n",
        "how to create a test loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS4iQtpdHNDj"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#def test_model(data_loader, criterion):"
      ],
      "metadata": {
        "id": "RHDaFPIMMgqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3IL0WlYJCx6"
      },
      "outputs": [],
      "source": [
        "#test_prediction ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7gfM8qzHNDk"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_YAL3g-96nx"
      },
      "source": [
        "# Saving the model weights for future use\n",
        "\n",
        "Refer to the example on saving files in the course Resources folder on github"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 9.2\n",
        "\n",
        "Create a new blank model using the same architecture you used from part 8. <br>\n",
        "Load the model weights from your previously trained model from part 8. <br> Finally, compare the predictions of the two models on the same datapoint."
      ],
      "metadata": {
        "id": "8Wmv0550Jwzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model(torch.from_numpy(X_test[42:43].copy()).float())"
      ],
      "metadata": {
        "id": "OtirZ6mCJnSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIMTsRwT96ny"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"my_pytorch_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SWQfT1ZVVB0"
      },
      "outputs": [],
      "source": [
        "#model_reloaded = #Insert the same Sequential model architecture from part 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrgCYxY596ny"
      },
      "outputs": [],
      "source": [
        "model_reloaded.load_state_dict(torch.load(\"my_pytorch_model\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeUU8hqO96ny"
      },
      "outputs": [],
      "source": [
        "model_reloaded(torch.from_numpy(X_test[42:43].copy()).float())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are these the same prediction?"
      ],
      "metadata": {
        "id": "WRm8G8snKjtI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHkqXnk5K0mu"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XCPKJtfK0mv"
      },
      "source": [
        "Task 9.2 answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMx5tfr8K0mv"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Layer Naming\n",
        "\n",
        "For particularly large models or new architectures, it can be helpful to name <br>\n",
        "model layers or blocks so that you can identify the source of bugs, etc."
      ],
      "metadata": {
        "id": "ptw8V26_HeXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict"
      ],
      "metadata": {
        "id": "3qrpIMH2ITvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_layer_naming = torch.nn.Sequential(OrderedDict([\n",
        "          ('simple_linear_layer_1', nn.Linear(10,100)),\n",
        "          ('a_neat_activation_function', nn.ReLU()),\n",
        "        ]))"
      ],
      "metadata": {
        "id": "q5Z9S79AHPpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_with_layer_naming)"
      ],
      "metadata": {
        "id": "II9-ZGUEIePf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_with_layer_naming.simple_linear_layer_1)"
      ],
      "metadata": {
        "id": "daj_e2toIQJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2TzmQLLHNDm"
      },
      "source": [
        "### Task 10 (Bonus)\n",
        " For `X_train[0]` perform the forward pass yourself using matrix <br> multiplications.\n",
        "Remember to include the biases. <br>\n",
        "Check with the prediction of the model that you get exactly the same! <br>\n",
        "\n",
        "Hints:\n",
        "- use `np.dot(x,y)` for matrix multiplication\n",
        "- for the first layer it would look like this:\n",
        "    * matrix mult: `X_new` dot `l1`\n",
        "    * add bias `b1`\n",
        "    * apply `relu(...)`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "CLblus2vA8vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKw_DLg3HNDm"
      },
      "outputs": [],
      "source": [
        "l1 = np.array(model[0].weight.data.numpy()).T\n",
        "b1 = np.array(model[0].bias.data.numpy())\n",
        "l2 = np.array(model[2].weight.data.numpy()).T\n",
        "b2 = np.array(model[2].bias.data.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a9dRAu9HNDm"
      },
      "outputs": [],
      "source": [
        "X_new = X_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n8T3Jp_HNDm"
      },
      "outputs": [],
      "source": [
        "X_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azxpa8UJHNDn"
      },
      "outputs": [],
      "source": [
        "l1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YIy1pfnHNDn"
      },
      "outputs": [],
      "source": [
        "b1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Va6L70U_HNDn"
      },
      "outputs": [],
      "source": [
        "# This is the entirety of the ReLU function. How cool is that!\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2J5yv18SHNDn"
      },
      "outputs": [],
      "source": [
        "model(torch.from_numpy(X_new).float().unsqueeze(0))   # reproduce this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cFbtYJrHNDn"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRFSwoyqkCFT"
      },
      "outputs": [],
      "source": [
        "#x =\n",
        "#print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPTqjAdiHNDn"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "nav_menu": {
      "height": "264px",
      "width": "369px"
    },
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}