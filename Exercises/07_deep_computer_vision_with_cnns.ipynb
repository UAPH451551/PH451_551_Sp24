{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UAPH451551/PH451_551_Sp24/blob/main/Exercises/07_deep_computer_vision_with_cnns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Computer Vision with Convolutional Neural Networks"
      ],
      "metadata": {
        "id": "OEWaY_SPoHjj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8lxARkCnyXL"
      },
      "source": [
        "**Chapter 14 – Deep Computer Vision Using Convolutional Neural Networks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcOmL14unyXN"
      },
      "source": [
        "❗️ **This will be very slow, unless you are using a GPU for the later code**\n",
        "\n",
        "❗️ **If you do not, then you should run this notebook in Colab, using a GPU runtime**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoHybjaRnyXO"
      },
      "source": [
        "File name convention: For group 42 and memebers Richard Stallman and Linus <br> Torvalds it would be: <br>\n",
        "\"07_Stallman_Torvalds.pdf\".\n",
        "\n",
        "Submission via blackboard (UA).\n",
        "\n",
        "Feel free to answer free text questions in text cells using markdown and <br>\n",
        "possibly $\\LaTeX{}$ if you want to.\n",
        "\n",
        "**You don't have to understand every line of code here and it is not intended** <br>\n",
        "**for you to try to understand every line of code.** <br>\n",
        "**Big blocks of code are usually meant to just be clicked through.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_18l9BDanyXQ"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxEXr8InnyXQ"
      },
      "outputs": [],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uccX_HkKnyXQ"
      },
      "outputs": [],
      "source": [
        "def plot_image(image):\n",
        "    plt.imshow(image, cmap=\"gray\", interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "def plot_color_image(image):\n",
        "    plt.imshow(image, interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's **import some data** to see how convolutional filters work. One is a scenic <br>\n",
        "image of china and the other is an image of a flower. The first thing we should <br>\n",
        "do is **normalize the pixels**."
      ],
      "metadata": {
        "id": "SeolOU5zoawQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCvhwaonnyXR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_sample_image\n",
        "\n",
        "# Load sample images\n",
        "china = load_sample_image(\"china.jpg\") / 255\n",
        "flower = load_sample_image(\"flower.jpg\") / 255\n",
        "images = np.array([china, flower])\n",
        "batch_size, height, width, channels = images.shape\n",
        "print(batch_size, height, width, channels)\n",
        "\n",
        "images = torch.from_numpy(images).permute(0, 3, 1, 2)\n",
        "\n",
        "plt.imshow(china)\n",
        "plt.axis(\"off\") # Not shown in the book\n",
        "plt.show()\n",
        "plt.imshow(flower)\n",
        "plt.axis(\"off\") # Not shown in the book\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next let's create some filters. Here we're creating image filters that have the <br>\n",
        "shape **7x7x3x2**. So that's a 7x7 grid which will pass over three color channels <br>\n",
        "and we have two filters for each of those dimensions. Here we want two filters <br>\n",
        "as the final dimension to demonstrate creating **vertical** and **horizontal** filters."
      ],
      "metadata": {
        "id": "S9ZSIHLoq2EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 2 filters\n",
        "filters = torch.zeros((2, channels, 7, 7), dtype=torch.float32)\n",
        "filters[0, :, :, 3] = 1  # vertical line\n",
        "filters[1, :, 3, :] = 1  # horizontal line\n",
        "plt.imshow(torch.moveaxis(filters[0,:,:,:], 0, 2))\n",
        "plt.show()\n",
        "plt.imshow(torch.moveaxis(filters[1,:,:,:], 0, 2))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ur47L9mapY2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that when we look at the shape of the outputs of our filters it now has <br>\n",
        "**final dimension 2 instead of 3**. What we've done here is **reduced our 3** red, <br>\n",
        "green, blue **(RGB) channels to two filter channels** that have picked out the <br>\n",
        "vertical and horizontal lines in all three color channels then added them up."
      ],
      "metadata": {
        "id": "FrNJ2acou-pV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(images.shape)\n",
        "print(filters.shape)"
      ],
      "metadata": {
        "id": "KERi-MID9Zk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nn.functional.conv2d(images.to(torch.float32), weight=filters, bias=None, stride=1, padding='same')\n",
        "print(outputs.shape)\n",
        "\n",
        "plt.imshow(outputs[0, 1, :, :], cmap='gray') # plot 1st image's 2nd feature map\n",
        "plt.axis(\"off\") # Not shown in the book\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cP5LjEVTu2Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_pCaX9pnyXR"
      },
      "outputs": [],
      "source": [
        "def crop(images):\n",
        "    try:\n",
        "      return images[:, 150:220, 130:250]\n",
        "    except:\n",
        "      return images[150:220, 130:250]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at our different color channels to see what an unfiltered image <br>\n",
        "looks like."
      ],
      "metadata": {
        "id": "e3lnBVYjvXBB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JA6Yb4lcnyXR"
      },
      "outputs": [],
      "source": [
        "plot_image(crop(images[0, 0, :, :]))\n",
        "plt.show()\n",
        "plot_image(crop(images[0, 1, :, :]))\n",
        "plt.show()\n",
        "plot_image(crop(images[0, 2, :, :]))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjSPlh8rnyXS"
      },
      "source": [
        "# Basics: Filters and Pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIS0_bPTnyXS"
      },
      "source": [
        "## Task 1: Filters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-c1L_PGInyXS"
      },
      "outputs": [],
      "source": [
        "for feature_map_index, filename in enumerate([\"china_vertical\", \"china_horizontal\"]):\n",
        "    plot_image(crop(outputs[0, feature_map_index, :, :]))\n",
        "    plt.title(filename)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaUjUD9bnyXS"
      },
      "source": [
        "**Task 1 a)**: Describe how the filters work and what their purpose in a CNN is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsnymzLuLvm2"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OklYadm7nyXT"
      },
      "source": [
        "**Task 1 a) answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L33GBuxILvm4"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AtRjDWfnyXT"
      },
      "source": [
        "## Convolutional Layer in Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-Z_teqrnyXT"
      },
      "source": [
        "To create a 2D convolutional layer use `nn.Conv2d` <br>\n",
        "(https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozcTzKjenyXT"
      },
      "source": [
        "**Task 1 b)**    \n",
        "Create a convolutional layer with 32 filters and `kernel_size` `(3,3)`. <br>\n",
        "Apply it to `images[0:1]` and explain the shape of the output. **Do not** <br>\n",
        "**explicitly pass any filters** this time. Instead, use the default random <br>\n",
        "initialization for pytorch convolutional layers. Run it a couple of times and <br>\n",
        "notice that you get a different image each time.\n",
        "\n",
        "You can plot the resulting images if you want (for example `plot_image` <br>\n",
        "`(new_images[0,0,:,:])` for the first filter)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lqXyHG0nyXT"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#conv2d_layer = nn.Conv2d(in_channels=...\n",
        "#im = images[0:1].to(torch.float32)\n",
        "#new_images = conv2d_layer(im).detach().numpy()\n",
        "#print(new_images.shape) #EXPLAIN THIS BELOW\n",
        "#plot_image(new_images[0,0,:,:])"
      ],
      "metadata": {
        "id": "a16ic-zCx5ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1b) shape explanation:"
      ],
      "metadata": {
        "id": "HyFUuZY1TxAo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YhMActnnyXT"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wydjADThnyXU"
      },
      "source": [
        "## Cropping the Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEkeGjS2nyXU"
      },
      "outputs": [],
      "source": [
        "cropped_images = np.array([crop(image) for image in images], dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "5q8ZciH-nyXU"
      },
      "outputs": [],
      "source": [
        "plot_image(cropped_images[0, 0, :, :])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcuZMTyUnyXU"
      },
      "source": [
        "## Task 2: Max Pooling Layer in Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBp3x3JwnyXU"
      },
      "source": [
        "Pooling layers are used to **shrink the input image** in order to reduce the <br> computational load, the memory usage, and the number of parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR0GdtTgnyXU"
      },
      "source": [
        "**Task 2 a)** <br>\n",
        "- Create a max pool layer of kernel_size=(2,2) <br>\n",
        "(https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)\n",
        "- apply the max pool layer to the `cropped_images` assigning the result to the <br>\n",
        "variable `output`\n",
        "- **Note:** Be sure to convert the input `cropped_images` to tensor and to <br>\n",
        "the right datatype beforehand using <br>\n",
        "`torch.from_numpy(cropped_images).to(torch.float32)` and use <br>\n",
        "`.detach().numpy()` afterward to convert your model output to numpy for <br> visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S6C3U3HnyXU"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud_iKXZ_nyXV"
      },
      "outputs": [],
      "source": [
        "# maxpool_layer = nn.MaxPool2d(..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-d7kOlyEnyXV"
      },
      "outputs": [],
      "source": [
        "# output ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76NSKunRnyXV"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1gey5ZonyXV"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(12, 8))\n",
        "gs = mpl.gridspec.GridSpec(nrows=1, ncols=2, width_ratios=[1, 1])\n",
        "\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "ax1.set_title(\"Input\", fontsize=14)\n",
        "ax1.imshow(np.moveaxis(cropped_images[0], 0, 2))  # plot the 1st image\n",
        "ax1.axis(\"off\")\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "ax2.set_title(\"Output\", fontsize=14)\n",
        "ax2.imshow(np.moveaxis(output[0], 0, 2))  # plot the output for the 1st image\n",
        "ax2.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1e9PI-0nyXV"
      },
      "outputs": [],
      "source": [
        "\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "gs = mpl.gridspec.GridSpec(nrows=1, ncols=2, width_ratios=[1, 1])\n",
        "\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "ax1.set_title(\"Input\", fontsize=14)\n",
        "ax1.imshow(np.moveaxis(cropped_images[1], 0, 2))\n",
        "ax1.axis(\"off\")\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "ax2.set_title(\"Output\", fontsize=14)\n",
        "ax2.imshow(np.moveaxis(output[1], 0, 2))\n",
        "ax2.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acpPMq18nyXV"
      },
      "source": [
        "**Task 2 b)**\n",
        "\n",
        "Describe the effect of the max pooling layer. What are its benefits for a <br> Neural Network? What are the downsides?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbQ1bl-4nyXW"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjsFK3_6nyXW"
      },
      "source": [
        "Task 2b) answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw6KiAHEnyXW"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3axR-JoGnyXW"
      },
      "source": [
        "# Tackling Fashion MNIST With a CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnC3XAPwnyXW"
      },
      "outputs": [],
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
        "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
        "\n",
        "# normalization\n",
        "X_mean = X_train.mean(axis=0, keepdims=True)\n",
        "X_std = X_train.std(axis=0, keepdims=True) + 1e-7\n",
        "X_train = (X_train - X_mean) / X_std\n",
        "X_valid = (X_valid - X_mean) / X_std\n",
        "X_test = (X_test - X_mean) / X_std\n",
        "\n",
        "\n",
        "#Notice that pytorch convolutional layers expect the 1-axis to be the channels\n",
        "#dimension whereas generally linear layers will act on the last axis.\n",
        "\n",
        "X_train = X_train[:, np.newaxis, ...]\n",
        "X_valid = X_valid[:, np.newaxis, ...]\n",
        "X_test = X_test[:, np.newaxis, ...]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X.copy()).float()\n",
        "        self.y = torch.from_numpy(y.copy()).long()\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_data = ClassificationDataset(X_train, y_train)\n",
        "valid_data = ClassificationDataset(X_valid, y_valid)\n",
        "test_data = ClassificationDataset(X_test, y_test)\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "cfnQJoll5FLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxEwZh2anyXW"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "DefaultConv2d = partial(nn.Conv2d,\n",
        "                        kernel_size=3, padding='same')\n",
        "\n",
        "model = nn.Sequential(\n",
        "    DefaultConv2d(in_channels=1, out_channels=64, kernel_size=7),\n",
        "    nn.MaxPool2d(kernel_size=(2,2)),\n",
        "    DefaultConv2d(in_channels=64, out_channels=128),\n",
        "    DefaultConv2d(in_channels=128, out_channels=128),\n",
        "    nn.MaxPool2d(kernel_size=(2,2)),\n",
        "    DefaultConv2d(in_channels=128, out_channels=256),\n",
        "    DefaultConv2d(in_channels=256, out_channels=256),\n",
        "    nn.MaxPool2d(kernel_size=(2,2)),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(in_features=64*36, out_features=128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(in_features=128, out_features=64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(in_features=64, out_features=10),\n",
        "    nn.Softmax(dim=-1)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7tky05bnyXW"
      },
      "source": [
        "### Visualization of Model Structure\n",
        "This is not necessary, but maybe interesting."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz"
      ],
      "metadata": {
        "id": "zc0QGe7pEGA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchviz import make_dot\n",
        "x = torch.randn(1,1,28,28)\n",
        "y = model(x)\n",
        "\n",
        "#make_dot generates an image of your model and .render() outputs it to a file.\n",
        "#Click the folder icon on the left side of colab and you should see a file\n",
        "#call model_image.png that shows the model\n",
        "make_dot(y.mean(), params=dict(model.named_parameters())).render(\"model_image\", format=\"png\")"
      ],
      "metadata": {
        "id": "wv-i93zUqAIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Testing Loops\n",
        "Note that compared to previous training loops this one has now introduced the <br>\n",
        "concept of a **\"device\"**. Here that is included so that you can use **GPU** for the <br>\n",
        "larger models in this notebook like ResNet. The correct way to use a device <br>\n",
        "is to **pass the model and data to the same device *before* doing operations**. <br>\n",
        "GPU's are a type of processor that are especially good at matrix-based <br>\n",
        "operations such as those used in graphics as well as machine learning."
      ],
      "metadata": {
        "id": "VfpN-IYA2mpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_validate(train_loader, val_loader, model, optimizer, criterion, num_epochs, metric=None, scheduler=None, device='cpu'):\n",
        "    history = {\n",
        "        'epoch': [],\n",
        "        'train_loss': [],\n",
        "        'train_metric': [],\n",
        "        'val_loss': [],\n",
        "        'val_metric': [],\n",
        "        'learning_rate': []\n",
        "    }  # Initialize a dictionary to store epoch-wise results\n",
        "\n",
        "    model.to(device)  # Move the model to the specified device\n",
        "\n",
        "    with torch.no_grad():\n",
        "        proper_dtype = torch.int64\n",
        "        X,y = next(iter(train_loader))\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        try:\n",
        "            loss = criterion(model(X), y.to(proper_dtype))\n",
        "        except:\n",
        "            try:\n",
        "                proper_dtype = torch.float32\n",
        "                loss = criterion(model(X), y.to(proper_dtype))\n",
        "            except:\n",
        "                print(\"No valid data-type could be found\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        epoch_loss = 0.0  # Initialize the epoch loss and metric values\n",
        "        epoch_metric = 0.0\n",
        "\n",
        "        # Training loop\n",
        "        for X, y in train_loader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            y = y.to(proper_dtype)\n",
        "            optimizer.zero_grad()  # Clear existing gradients\n",
        "            outputs = model(X)  # Make predictions\n",
        "            loss = criterion(outputs, y)  # Compute the loss\n",
        "            loss.backward()  # Compute gradients\n",
        "            optimizer.step()  # Update model parameters\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # THESE LINES HAVE BEEN UPDATED TO ACCOUNT FOR DEFAULT ARGUMENTS\n",
        "            if metric is not None:\n",
        "                epoch_metric += metric(outputs, y)\n",
        "            else:\n",
        "                epoch_metric += 0.0\n",
        "\n",
        "        # Average training loss and metric\n",
        "        epoch_loss /= len(train_loader)\n",
        "        epoch_metric /= len(train_loader)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        with torch.no_grad():  # Disable gradient calculation\n",
        "            val_loss = 0.0\n",
        "            val_metric = 0.0\n",
        "            for X_val, y_val in val_loader:\n",
        "                X_val = X_val.to(device)\n",
        "                y_val = y_val.to(device)\n",
        "                y_val = y_val.to(proper_dtype)\n",
        "                outputs_val = model(X_val)  # Make predictions\n",
        "                val_loss += criterion(outputs_val, y_val).item()  # Compute loss\n",
        "                if metric is not None:\n",
        "                    val_metric += metric(outputs_val, y_val)\n",
        "                else:\n",
        "                    val_metric += 0.0\n",
        "\n",
        "            val_loss /= len(val_loader)\n",
        "            val_metric /= len(val_loader)\n",
        "\n",
        "        # Append epoch results to history\n",
        "        history['epoch'].append(epoch)\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "        history['train_metric'].append(epoch_metric)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_metric'].append(val_metric)\n",
        "        history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, '\n",
        "              f'Train Metric: {epoch_metric:.4f}, Val Loss: {val_loss:.4f}, '\n",
        "              f'Val Metric: {val_metric:.4f}')\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "    return history, model"
      ],
      "metadata": {
        "id": "bahAURuP2lV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, data_loader, criterion, metric=None, device='cpu'):\n",
        "    model.to(device)  # Move the model to the specified device\n",
        "\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    total_loss = 0.0  # Initialize the total loss and metric values\n",
        "    total_metric = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        proper_dtype = torch.int64\n",
        "        X,y = next(iter(data_loader))\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        try:\n",
        "            loss = criterion(model(X), y.to(proper_dtype))\n",
        "        except:\n",
        "            try:\n",
        "                proper_dtype = torch.float32\n",
        "                loss = criterion(model(X), y.to(proper_dtype))\n",
        "            except:\n",
        "                print(\"No valid data-type could be found\")\n",
        "\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient tracking\n",
        "        for batch in data_loader:\n",
        "            X, y = batch\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            y = y.to(proper_dtype)\n",
        "            # Pass the data to the model and make predictions\n",
        "            outputs = model(X)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "            # Add the loss and metric for the batch to the total values\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if metric is not None:\n",
        "                total_metric += metric(outputs, y)\n",
        "            else:\n",
        "                total_metric += 0.0\n",
        "\n",
        "    # Average loss and metric for the entire dataset\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    avg_metric = total_metric / len(data_loader)\n",
        "\n",
        "    print(f'Test Loss: {avg_loss:.4f}, Test Metric: {avg_metric:.4f}')\n",
        "\n",
        "    return avg_loss, avg_metric"
      ],
      "metadata": {
        "id": "gyBwLFzh2uo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_metric(pred, target):\n",
        "    if len(pred.shape) == 1:\n",
        "        accuracy = torch.sum(torch.eq(pred > 0.5, target)).item() / len(pred)\n",
        "    else:\n",
        "        pred = pred.argmax(dim=1)\n",
        "        accuracy = torch.sum(pred == target).item() / len(pred)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "h5S52iA53Cl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPU Time:\n",
        "**If you haven't enabled GPU in your colab notebook, now is the time to do so.** <br>\n",
        "Only one group member should be working with GPU at a time as you will each <br>\n",
        "have a limit on how often and for how long colab will allow you to use gpu."
      ],
      "metadata": {
        "id": "ttNQcv9rJSuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "X2hW3OW2cBlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device) # should be cuda if Colab is connected to gpu"
      ],
      "metadata": {
        "id": "Qf-n-9P5JgEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zolfu3RfnyXX"
      },
      "source": [
        "## Task 3:\n",
        "- Train the model using `nn.CrossEntropyLoss` as `loss`, <br> `torch.optim.NAdam` as optimizer with `lr=2e-4`, and `\"accuracy_metric\"` for `metric` <br>\n",
        "- fit the model for 20 epochs using `train_loader` and 'valid_loader'<br>\n",
        "- `evaluate` the model on `test_loader` <br>\n",
        "- predict the first 20 instances of `X_test` and compare them to `y_test`<br>\n",
        "  - **Note:** Remember to convert `X_test` to tensor first using `torch.from_numpy()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKvtUt3bnyXY"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bm3KrClqnyXY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDBWC05enyXY"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1yFXkwXnyXY"
      },
      "source": [
        "## Task 4: ResNet-34\n",
        "\n",
        "ResNet is built on the idea of using **residual connections** between early layers <br>\n",
        "and later layers that the early layers are **not directly attached** to. In effect, <br>\n",
        "this results in **more total connections** in the neural network without actually <br>\n",
        "having to add additional weights to the model.\n",
        "\n",
        "Imagine you had a function that required **100 coefficients with 10 operations**. <br>\n",
        "Instead, you realize that many of the coefficients are **related** to one another <br>\n",
        "so you decide to **recycle terms** and  instead **include more recursive operations**. <br>\n",
        "Now your function has **20 coefficients** but you're doing **30 operations**.\n",
        "\n",
        "\n",
        "This is the idea of ResNet. We **reuse the same weights** multiple times but <br>\n",
        "connecting them to **different layers** each time. This can lead to models that <br>\n",
        "are on the order of 5+ times smaller without meaningfully reducing performance."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Pytorch Implementation of Resnet\n",
        "The following is pytorch's highly-optimized implementation <br>\n",
        "of resnet. However, we'll need to modify it slightly to get <br>\n",
        "the channel and target dimensions to match our problem."
      ],
      "metadata": {
        "id": "yRcqvNsbPs7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "from typing import Any, Callable, List, Optional, Type, Union\n",
        "\n",
        "from torch import Tensor\n",
        "\n",
        "from torchvision.transforms._presets import ImageClassification\n",
        "from torchvision.utils import _log_api_usage_once\n",
        "from torchvision.models._api import register_model, Weights, WeightsEnum\n",
        "from torchvision.models._meta import _IMAGENET_CATEGORIES\n",
        "from torchvision.models._utils import _ovewrite_named_param, handle_legacy_interface"
      ],
      "metadata": {
        "id": "H4T0y2OkOgjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=dilation,\n",
        "        groups=groups,\n",
        "        bias=False,\n",
        "        dilation=dilation,\n",
        "    )\n",
        "\n",
        "\n",
        "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion: int = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        base_width: int = 64,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\" https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion: int = 4\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        base_width: int = 64,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.0)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "EaXXV43xPCdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        block: Type[Union[BasicBlock, Bottleneck]],\n",
        "        layers: List[int],\n",
        "        input_channels: int = 3,\n",
        "        num_classes: int = 1000,\n",
        "        zero_init_residual: bool = False,\n",
        "        groups: int = 1,\n",
        "        width_per_group: int = 64,\n",
        "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        _log_api_usage_once(self)\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\n",
        "                \"replace_stride_with_dilation should be None \"\n",
        "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
        "            )\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(input_channels, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck) and m.bn3.weight is not None:\n",
        "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
        "                elif isinstance(m, BasicBlock) and m.bn2.weight is not None:\n",
        "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
        "\n",
        "    def _make_layer(\n",
        "        self,\n",
        "        block: Type[Union[BasicBlock, Bottleneck]],\n",
        "        planes: int,\n",
        "        blocks: int,\n",
        "        stride: int = 1,\n",
        "        dilate: bool = False,\n",
        "    ) -> nn.Sequential:\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
        "            )\n",
        "        )\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.inplanes,\n",
        "                    planes,\n",
        "                    groups=self.groups,\n",
        "                    base_width=self.base_width,\n",
        "                    dilation=self.dilation,\n",
        "                    norm_layer=norm_layer,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "def _resnet(\n",
        "    block: Type[Union[BasicBlock, Bottleneck]],\n",
        "    layers: List[int],\n",
        "    weights: Optional[WeightsEnum],\n",
        "    progress: bool,\n",
        "    **kwargs: Any,\n",
        ") -> ResNet:\n",
        "    if weights is not None:\n",
        "        _ovewrite_named_param(kwargs, \"num_classes\", len(weights.meta[\"categories\"]))\n",
        "\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "\n",
        "    if weights is not None:\n",
        "        model.load_state_dict(weights.get_state_dict(progress=progress, check_hash=True))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "WLrUPskrO3cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__all__ = [\n",
        "    \"ResNet\",\n",
        "    \"ResNet34_Weights\",\n",
        "    \"resnet34_modified\",\n",
        "    ]\n",
        "\n",
        "_COMMON_META = {\n",
        "    \"min_size\": (1, 1),\n",
        "    \"categories\": _IMAGENET_CATEGORIES,\n",
        "}\n",
        "\n",
        "class ResNet34_Weights(WeightsEnum):\n",
        "    IMAGENET1K_V1 = Weights(\n",
        "        url=\"https://download.pytorch.org/models/resnet34-b627a593.pth\",\n",
        "        transforms=partial(ImageClassification, crop_size=224),\n",
        "        meta={\n",
        "            **_COMMON_META,\n",
        "            \"num_params\": 21797672,\n",
        "            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#resnet\",\n",
        "            \"_metrics\": {\n",
        "                \"ImageNet-1K\": {\n",
        "                    \"acc@1\": 73.314,\n",
        "                    \"acc@5\": 91.420,\n",
        "                }\n",
        "            },\n",
        "            \"_ops\": 3.664,\n",
        "            \"_file_size\": 83.275,\n",
        "            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a simple training recipe.\"\"\",\n",
        "        },\n",
        "    )\n",
        "    DEFAULT = IMAGENET1K_V1\n",
        "\n",
        "def _resnet34_modified(input_channels: int, num_classes: int, block: Type[Union[BasicBlock, Bottleneck]], layers: List[int], weights: Optional[WeightsEnum], progress: bool, **kwargs: Any) -> ResNet:\n",
        "    if weights is not None:\n",
        "        _ovewrite_named_param(kwargs, \"num_classes\", len(weights.meta[\"categories\"]))\n",
        "\n",
        "    model = ResNet(block, layers, input_channels=input_channels, num_classes=num_classes, **kwargs)\n",
        "\n",
        "    if weights is not None:\n",
        "        # Load state dict but ignore first conv layer if number of input channels is not 3\n",
        "        state_dict = weights.get_state_dict(progress=progress, check_hash=True)\n",
        "        if input_channels != 3:\n",
        "            state_dict.pop('conv1.weight', None)\n",
        "        model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet34_modified(input_channels: int, num_classes: int, *, weights: Optional[ResNet34_Weights] = None, progress: bool = True, **kwargs: Any) -> ResNet:\n",
        "    return _resnet34_modified(input_channels, num_classes, BasicBlock, [3, 4, 6, 3], weights, progress, **kwargs)"
      ],
      "metadata": {
        "id": "YJXu2spbOOpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Modified version of resnet\n",
        "This is our modified version of resnet which has had the <br>\n",
        "input channels and output target classes modified so as to <br>\n",
        "be manually adjustable for our needs."
      ],
      "metadata": {
        "id": "JY_n4xiVQF8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _resnet34_modified(input_channels: int, num_classes: int, block: Type[Union[BasicBlock, Bottleneck]], layers: List[int], weights: Optional[WeightsEnum], progress: bool, **kwargs: Any) -> ResNet:\n",
        "    if weights is not None:\n",
        "        _ovewrite_named_param(kwargs, \"num_classes\", len(weights.meta[\"categories\"]))\n",
        "\n",
        "    model = ResNet(block, layers, input_channels=input_channels, num_classes=num_classes, **kwargs)\n",
        "\n",
        "    if weights is not None:\n",
        "        # Load state dict but ignore first conv layer if number of input channels is not 3\n",
        "        state_dict = weights.get_state_dict(progress=progress, check_hash=True)\n",
        "        if input_channels != 3:\n",
        "            state_dict.pop('conv1.weight', None)\n",
        "        model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet34_modified(input_channels: int, num_classes: int, *, weights: Optional[ResNet34_Weights] = None, progress: bool = True, **kwargs: Any) -> ResNet:\n",
        "    return _resnet34_modified(input_channels, num_classes, BasicBlock, [3, 4, 6, 3], weights, progress, **kwargs)"
      ],
      "metadata": {
        "id": "lx5t01RxQFJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "pvq_Ev3S34JP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = resnet34_modified(input_channels=1, num_classes=10)"
      ],
      "metadata": {
        "id": "qlNIoVdLN6tE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8F-ioI1nyXZ"
      },
      "outputs": [],
      "source": [
        "print(resnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1fibOQGnyXZ"
      },
      "source": [
        "**Task 4:**   <br>\n",
        "a) Train the ResNet-34 model with Adam optimizer and train 10 for epochs <br>\n",
        "b) Compare the performance the results with the ones from Task 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEJZkKc1nyXZ"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kb0DLVEdnyXZ"
      },
      "outputs": [],
      "source": [
        "# initialize optimizer, etc.\n",
        "# fit the model to the training set\n",
        "# evaluate the model on the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNAoHlRHnyXZ"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnZ8DG4lnyXZ"
      },
      "source": [
        "## Task 5: Pretrained Models for Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ8LWxR6nyXa"
      },
      "source": [
        "In this section we follow loosely the pytorch <br>\n",
        "[Transfer Learning](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) example written by Sasank Chilamkurthy.<br>\n",
        "We'll be using a bee/ant classification dataset.\n",
        "\n",
        "Here we'll show the effects on model performance when using <br>\n",
        "a model which has weights **pretrained on a general dataset** <br>\n",
        "as compared with a model which is **trained from scratch**. In <br>\n",
        "this case we'll be looking at the same resnset34 from above <br>\n",
        "but with pretrained model weights.\n",
        "\n",
        "**These models may take over an hour to train if not on GPU.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet34\n",
        "from torchvision import datasets, models, transforms"
      ],
      "metadata": {
        "id": "oEfGe-VgUUOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://download.pytorch.org/tutorial/hymenoptera_data.zip"
      ],
      "metadata": {
        "id": "jPovcmrwUpKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip hymenoptera_data.zip"
      ],
      "metadata": {
        "id": "VQkq3I_RVVyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ],
      "metadata": {
        "id": "AWeGrbcXWCNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'hymenoptera_data'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "fJlEjoT_V3Dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(inp, title=None):\n",
        "    \"\"\"Display image for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ],
      "metadata": {
        "id": "v02DtG4FWiLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Notice that resnet by default is designed for 1000 classes so we change that to 2\n",
        "resnet_untrained = nn.Sequential(\n",
        "    resnet34(pretrained=False),\n",
        "    nn.Linear(1000, 2)\n",
        ")\n",
        "print(resnet_untrained)"
      ],
      "metadata": {
        "id": "FEiTEqDIYQvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(resnet_untrained.parameters(), lr=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_epochs = 20\n",
        "history, resnet_untrained = train_and_validate(dataloaders['train'], dataloaders['val'], resnet_untrained, optimizer, criterion, num_epochs, metric=accuracy_metric, scheduler=None, device=device)"
      ],
      "metadata": {
        "id": "9vOU_JI-XOPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_pretrained = nn.Sequential(\n",
        "    resnet34(pretrained=True),\n",
        "    nn.Linear(1000, 2)\n",
        ")"
      ],
      "metadata": {
        "id": "FEVSA6fJYU8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(resnet_pretrained.parameters(), lr=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_epochs = 20\n",
        "history, resnet_pretrained = train_and_validate(dataloaders['train'], dataloaders['val'], resnet_pretrained, optimizer, criterion, num_epochs, metric=accuracy_metric, scheduler=None, device=device)"
      ],
      "metadata": {
        "id": "FQGYWMOmYVy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bJitAwhnyXc"
      },
      "source": [
        "**Task 5:**\n",
        "- Task 5a) Explain transfer learning and its benefits\n",
        "- Task 5b) Compare the two trainings above (with/without base model trainable). <br>\n",
        "What is the difference and which one performs better here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Gf_t0f7nyXc"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXvb-PgunyXc"
      },
      "source": [
        "Task 5a) answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayfMeBtvnyXc"
      },
      "source": [
        "Task 5b) answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cwj0IInnnyXc"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEJwz1R8nyXc"
      },
      "source": [
        "# Task 6: High Accuracy CNN for MNIST\n",
        "Build your own CNN and try to achieve the highest possible accuracy on MNIST. <br>\n",
        "A basic structure is given below, play around with it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQUWJpQPnyXc"
      },
      "source": [
        "Try a model which uses 2 convolutional layers, followed by 1 pooling layer, <br>\n",
        "then dropout 25%, then a Linear layer, another dropout layer but with 50% <br> dropout, and finally the output layer. It reaches about 99.2% accuracy on the <br>\n",
        "test set. This places this model roughly in the top 20% in the <br>\n",
        "[MNIST Kaggle competition](https://www.kaggle.com/c/digit-recognizer/).\n",
        "\n",
        "In order to reach an accuracy higher than 99.5% on the test set you might try:\n",
        "\n",
        "a) batch normalization layers<br>\n",
        "b) set a learning scheduler (Check Chapter 11)<br>\n",
        "c) add image augmentation (Check Chapter 14)<br>\n",
        "d) create an ensemble (Check Chapter 14)<br>\n",
        "e) use hyperparameter tuning\n",
        "\n",
        "As long as you implement at least **two** of the above you will get full points <br>\n",
        "on this one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFfeMKtKnyXd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "nav_menu": {},
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}