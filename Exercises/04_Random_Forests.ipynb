{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UAPH451551/PH451_551_Sp24/blob/main/Exercises/04_Random_Forests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KewENlfhEt8U"
      },
      "source": [
        "# Hands-On #4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD61NvGQEt8W"
      },
      "source": [
        "**Chapter 7 – Ensemble Learning and Random Forests**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEION-2wEt8X"
      },
      "source": [
        "File name convention: For group 42 and memebers Richard Stallman and Linus <br>\n",
        "Torvalds it would be\n",
        "\"04_Random_Forests_Stallman_Torvalds.pdf\".\n",
        "\n",
        "Submission via blackboard (UA).\n",
        "\n",
        "Feel free to answer free text questions in text cells using markdown and <br> possibly $\\LaTeX{}$ if you want to.\n",
        "\n",
        "**You don't have to understand every line of code here and it is not intended <br> for you to try to understand every line of code.   <br>\n",
        "Big blocks of code are usually meant to just be clicked through.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9wFyvoIEt8Y"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXkHonSQEt8Y"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You don't need to understand the next 3 code blocks**\n",
        "\n",
        "Let's create some functions we'll need for the following tasks. `plot_digit` <br>\n",
        "plots a 28x28 image as if it were a heatmap. `plot_decision_boundary` plots <br>\n",
        "a decision boundary as we saw last week. `plot_predictions` plots predictions <br>\n",
        "made by one regressor or the sum of multiple regressors on a single plot."
      ],
      "metadata": {
        "id": "6UC17BK_GKSU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H5Ly8MCEt8a"
      },
      "outputs": [],
      "source": [
        "def plot_digit(data):\n",
        "    image = data.reshape(28, 28)\n",
        "    plt.imshow(image, cmap = mpl.cm.hot,\n",
        "               interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmNuaFqjEt8b"
      },
      "outputs": [],
      "source": [
        "def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.45, -1, 1.5],\n",
        "                           alpha=0.5, contour=True):\n",
        "    x1s = np.linspace(axes[0], axes[1], 100)\n",
        "    x2s = np.linspace(axes[2], axes[3], 100)\n",
        "    x1, x2 = np.meshgrid(x1s, x2s)\n",
        "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
        "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
        "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
        "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
        "    if contour:\n",
        "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
        "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
        "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", alpha=alpha)\n",
        "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", alpha=alpha)\n",
        "    plt.axis(axes)\n",
        "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
        "    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOkdoyU8Et8b"
      },
      "outputs": [],
      "source": [
        "def plot_predictions(regressors, X, y, axes, label=None, style=\"r-\",\n",
        "                     data_style=\"b.\", data_label=None):\n",
        "    x1 = np.linspace(axes[0], axes[1], 500)\n",
        "    y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors)\n",
        "    plt.plot(X[:, 0], y, data_style, label=data_label)\n",
        "    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n",
        "    if label or data_label:\n",
        "        plt.legend(loc=\"upper center\", fontsize=16)\n",
        "    plt.axis(axes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3IfUqfiEt8b"
      },
      "source": [
        "### Task 1\n",
        "\n",
        "**Task 1a)**   \n",
        "As we did in the [last assignment](https://github.com/UAPH451551/PH451_551_Sp23/blob/main/Exercises/03_Decision_Trees.ipynb), load the [moon dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) with 500 <br>\n",
        "samples, 0.3 for the noise value, and `random_state` to `42` . Also, perform <br> train_test_split on it. Use the default split value of 25% test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMsvmlFYEt8c"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_moons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzACHvMk2hd6"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyHuNlzLEt8c"
      },
      "outputs": [],
      "source": [
        "# X, y =\n",
        "# X_train, X_test, y_train, y_test ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCHZcxPr2tqY"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVSomnGOEt8d"
      },
      "source": [
        "Now, we are going to build a voting classifier with the following estimators:   \n",
        "- logistic regression (`solver=\"lbfgs\", random_state=42`)\n",
        "- random forest classifier (`n_estimators=100, random_state=42`)\n",
        "- svc (`gamma=\"scale\", random_state=42`)\n",
        "    \n",
        "**Task 1b)**   \n",
        "Initialize those three objects with the given parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prXOu3zZEt8d"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmY3nOYJEt8d"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqfGWAabEt8d"
      },
      "outputs": [],
      "source": [
        "# log_clf =\n",
        "# rnd_clf =\n",
        "# svm_clf ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a69vTEWnEt8d"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cqBFMubEt8e"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import VotingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTpyKevcEt8e"
      },
      "outputs": [],
      "source": [
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='hard')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FciFbLijEt8e"
      },
      "source": [
        "**Task 1c)**  \n",
        "Explain `voting='hard\"` in the [VotingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html). What is soft voting?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gti-kixREt8e"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk6XgBzpEt8e"
      },
      "source": [
        "Task 1c) answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wenRrerEt8f"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfLeIJXwEt8f"
      },
      "source": [
        "**Task 1d)**  \n",
        "Finally, fit the voting classifier to your train dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZBIMcsQEt8f"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqQqYv7JEt8f"
      },
      "outputs": [],
      "source": [
        "# fit here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap-QMoRMEt8f"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l7G78gVEt8f"
      },
      "source": [
        "### Task 2\n",
        "\n",
        "Now we are going to print out the accuracy scores for each of the classifier <br>\n",
        "above.\n",
        "\n",
        "Complete the code below as indicated in the comments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_rnTmXSEt8f"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ux1_oHxWEt8g"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "    clf.fit(X_train, y_train)\n",
        "    # predict the y_test with each classifier\n",
        "    #y_pred =\n",
        "    # calculate the accuracy score for each\n",
        "    # classifier_accuracy_score =\n",
        "    # finally, we will print them out\n",
        "    print(clf.__class__.__name__, classifier_accuracy_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Es2kbeWTEt8g"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTMPaK6AEt8g"
      },
      "source": [
        "### Task 3: Soft voting\n",
        "\n",
        "**Task 3a)**  \n",
        "Now  repeat everything in Tasks 1 and 2 but for soft voting. To do that, just <br>\n",
        "change the value in the ```voting``` parameter to `soft` which will implement <br>\n",
        "probability-based voting. You will also need to set `probability=True` in the <br>\n",
        "SVC model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb7Y4C0aEt8g"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkE0pYMgEt8h"
      },
      "outputs": [],
      "source": [
        "# fit to a new voting classifier with the 'soft' parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q23JfUcrEt8h"
      },
      "outputs": [],
      "source": [
        "# calculate the accuracy scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-86-y7J0Et8h"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4YY85aAEt8i"
      },
      "source": [
        "**Task 3b)**  \n",
        "How are the results different?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TQsIQj3Et8i"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6MwXVtwEt8i"
      },
      "source": [
        "Task 3b) answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQdDtCaOEt8j"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb6HLwKdEt8j"
      },
      "source": [
        "# Bagging ensembles\n",
        "\n",
        "In bagging ensembles we essentially **train multiple classifiers on subsets of** <br>\n",
        "**our total dataset**. The key characteristic about \"bagging\" is that we **draw** <br>\n",
        "**samples with replacement**. That is, we can potentially have multiple different <br>\n",
        "models train on some small amount of overlapping data. The setting for that <br>\n",
        "is `bootstrap=True`. Below, we'll create a bagginge ensemble of decision trees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvgFri-DEt8j"
      },
      "source": [
        "### Task 4 Bagging Classifier\n",
        "\n",
        "**Task 4a)**  \n",
        "- Initialize a [bagging classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) for a decision tree classifier with the <br> following parameters:\n",
        "    + `n_estimators = 500`\n",
        "    + `max_samples = 100`\n",
        "    + `bootstrap = True`\n",
        "    + `random_state = 42`\n",
        "\n",
        "- Fit the bagging classifier to the training data and make a prediction.  \n",
        "- Calculate the [accuracy score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnnzVMSxEt8j"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcRCgBt1Et8j"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzXyy5mQEt8j"
      },
      "outputs": [],
      "source": [
        "# bag_clf ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5flz2TXCEt8j"
      },
      "outputs": [],
      "source": [
        "# fit the classifier and calculate the prediction.\n",
        "#\n",
        "# y_pred ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQpg-0iuEt8k"
      },
      "outputs": [],
      "source": [
        "# bag_clr_accuracy_score =\n",
        "# bag_clr_accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJOGJHzdEt8k"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3IbKERuEt8k"
      },
      "source": [
        "**Task 4b**  \n",
        "Do the same for a Decision Tree Classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tav3s8tVEt8k"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5I_hLoIZEt8k"
      },
      "outputs": [],
      "source": [
        "#tree_clf =\n",
        "\n",
        "# fit\n",
        "# make prediction\n",
        "# y_pred_tree =\n",
        "# pred_tree_accuracy_score ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHesP7N-Et8k"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvvY0ui9Et8l"
      },
      "outputs": [],
      "source": [
        "fix, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True)\n",
        "plt.sca(axes[0])\n",
        "plot_decision_boundary(tree_clf, X, y)\n",
        "plt.title(\"Decision Tree\", fontsize=14)\n",
        "plt.sca(axes[1])\n",
        "plot_decision_boundary(bag_clf, X, y)\n",
        "plt.title(\"Decision Trees with Bagging\", fontsize=14)\n",
        "plt.ylabel(\"\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs-Vdr7_LgoY"
      },
      "source": [
        "**Task 4c**  \n",
        "Based only on the decision boundary plots above, which model is more likely to  <br>\n",
        "be overfit: bagging ensemble classifier or decision tree classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hErI_pu1LgoZ"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4c) answer:"
      ],
      "metadata": {
        "id": "uDKheHdmLb_N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-4Qj9G0LgoZ"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TKn8XMwEt8m"
      },
      "source": [
        "## Feature importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AznE5TIsEt8m"
      },
      "source": [
        "Let's use the [mnist](https://www.openml.org/d/554) dataset. It contains 60k handwritten digits <br>\n",
        "for training and 10k for testing. Here we're going to examine what features <br>\n",
        "of a handwritten character a random forest identifies as important."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcwNi1LEEt8m"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "mnist.target = mnist.target.astype(np.uint8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nrUQybFEt8m"
      },
      "source": [
        "### Task 5: Feature Importance\n",
        "\n",
        "**Task 5a)**  \n",
        "Initialize a random forest classifier with 100 estimators. <br>\n",
        "Fit it to mnist[\"data\"] and mnist[\"target\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqBg5bgxEt8m"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btiDM4O2Et8m"
      },
      "outputs": [],
      "source": [
        "#rand_forest_clf =\n",
        "# fit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bq7k25KEt8n"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMB-V22KEt8n"
      },
      "outputs": [],
      "source": [
        "plot_digit(rand_forest_clf.feature_importances_)\n",
        "\n",
        "cbar = plt.colorbar(ticks=[rand_forest_clf.feature_importances_.min(), rand_forest_clf.feature_importances_.max()])\n",
        "cbar.ax.set_yticklabels(['Not important', 'Very important'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgI8F4uDEt8n"
      },
      "source": [
        "**Task 5b)**  \n",
        "What can you infer about the model and dataset from the visualization above?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw7jambCEt8n"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VnjkeR1Et8n"
      },
      "source": [
        "Task 5b) answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK8baCtkEt8n"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNwJl5n6Et8o"
      },
      "source": [
        "# AdaBoost\n",
        "\n",
        "AdaBoost is an approach that entails training a classifier on a dataset, <br>\n",
        "evaluating which datapoints it struggles with, then **trains copies of the** <br>\n",
        "**classifier with the events reweighted** such that data points it struggles with <br>\n",
        "are given higher priority during training. Importantly, **each individual** <br>\n",
        "**classifier is likely to perform worse** on validation/test data than the original <br>\n",
        "but the **ensemble of the original and copies are likely to perform better**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WX-UIR90Et8o"
      },
      "source": [
        "### Task 6: AdaBoost\n",
        "Now, initialize an [AdaBoost classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) for a decision tree classifier with the <br>\n",
        "following parameters:   \n",
        "- max_depth = 1 - Decision Tree classifier parameter\n",
        "- n_estimators = 200 - Ada Boost classifier parameter\n",
        "- learning_rate = 0.5 - Ada Boost classifier parameter\n",
        "- algorithm = \"SAMME.R\" - Ada Boost classifier parameter\n",
        "- random_state = 42 - Ada Boost classifier parameter\n",
        "\n",
        "Fit it to the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyyfE7N5Et8o"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxy519e_Et8o"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "#ada_clf =\n",
        "#fit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elLKsgF2Et8p"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_MU_pNPEt8p"
      },
      "source": [
        "Plot the decision boundary for ada_clf, X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-o7MiELEt8p"
      },
      "outputs": [],
      "source": [
        "plot_decision_boundary(ada_clf, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Don't worry about understanding the following code. The important thing is to** <br>\n",
        "**understand the plots**.\n",
        "\n",
        "The following is essentially implementing the AdaBoost procedure from scratch <br>\n",
        "for a support vector classifier (SVC)."
      ],
      "metadata": {
        "id": "LaxWt_YRSBGk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sj2ejiLCEt8p"
      },
      "outputs": [],
      "source": [
        "m = len(X_train)\n",
        "\n",
        "fix, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True)\n",
        "for subplot, learning_rate in ((0, 1), (1, 0.5)):\n",
        "    sample_weights = np.ones(m)\n",
        "    plt.sca(axes[subplot])\n",
        "    for i in range(5):\n",
        "        svm_clf = SVC(kernel=\"rbf\", C=0.05, gamma=\"scale\", random_state=42)\n",
        "        svm_clf.fit(X_train, y_train, sample_weight=sample_weights)\n",
        "        y_pred = svm_clf.predict(X_train)\n",
        "        sample_weights[y_pred != y_train] *= (1 + learning_rate)\n",
        "        plot_decision_boundary(svm_clf, X, y, alpha=0.2)\n",
        "        plt.title(\"learning_rate = {}\".format(learning_rate), fontsize=16)\n",
        "    if subplot == 0:\n",
        "        plt.text(-0.7, -0.65, \"1\", fontsize=14)\n",
        "        plt.text(-0.6, -0.10, \"2\", fontsize=14)\n",
        "        plt.text(-0.5,  0.10, \"3\", fontsize=14)\n",
        "        plt.text(-0.4,  0.55, \"4\", fontsize=14)\n",
        "        plt.text(-0.3,  0.90, \"5\", fontsize=14)\n",
        "    else:\n",
        "        plt.ylabel(\"\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The properties of this form of ensemble classifier look like the following."
      ],
      "metadata": {
        "id": "88ZY9cugS76L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBk25am1Et8p"
      },
      "outputs": [],
      "source": [
        "list(m for m in dir(ada_clf) if not m.startswith(\"_\") and m.endswith(\"_\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WZSJ0J2Et8q"
      },
      "source": [
        "# Gradient Boosting\n",
        "\n",
        "Gradient boosting is a technique whereby we build an ensemble of models where <br>\n",
        "subsequent models try to slightly correct the errors of previous models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-28mSAhEt8r"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) - 0.5\n",
        "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TESGxiaEt8r"
      },
      "outputs": [],
      "source": [
        "plt.scatter(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGtSqIDTEt8r"
      },
      "source": [
        "### Task 7: Gradient Boosting\n",
        "\n",
        "**Task 7a)**  \n",
        "In this task, we will be doing the following.  <br>\n",
        "1. Initialize a decision tree regressor with `max_depth=2` and <br> `random_state=42`.\n",
        "2. Fit it to X and y.\n",
        "3. Substract the predicted y from the y values you fitted to in step 2.\n",
        "4. Fit the new classifier with the same parameters for the value in step 3.\n",
        "5. Repeat this one more time. You should obtain values for y2 and y3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fcr2KIDlEt8r"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3YFkPuGEt8r"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "#tree_reg1 =\n",
        "# fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cTeuxlEEt8s"
      },
      "outputs": [],
      "source": [
        "#y2 =\n",
        "#tree_reg2 =\n",
        "#fit to X and y2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JoyrRX7Et8s"
      },
      "outputs": [],
      "source": [
        "#y3 =\n",
        "#tree_reg3 =\n",
        "#fit to X and y3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te-UHncaEt8s"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgvpZCLoEt8s"
      },
      "outputs": [],
      "source": [
        "X_new = np.array([[0.8]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghmW3pFbEt8s"
      },
      "source": [
        "The new predicted `y` should be the sum of the predictions for the `X_new` <br> performed by each tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "490iIZh6Et8s"
      },
      "outputs": [],
      "source": [
        "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myADsyZFEt8s"
      },
      "outputs": [],
      "source": [
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OjYk1_EEt8t"
      },
      "source": [
        "**Task 7b)**  \n",
        "What do plots below show?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CDqsNMvEt8t"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn_eFP40Et8t"
      },
      "source": [
        "Task 7b) answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drAt-s07Et8t"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaWi1IP8Et8t"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(11,11))\n",
        "\n",
        "plt.subplot(321)\n",
        "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h_1(x_1)$\", style=\"g-\", data_label=\"Training set\")\n",
        "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
        "plt.title(\"Residuals and tree predictions\", fontsize=16)\n",
        "\n",
        "plt.subplot(322)\n",
        "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1)$\", data_label=\"Training set\")\n",
        "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
        "plt.title(\"Ensemble predictions\", fontsize=16)\n",
        "\n",
        "plt.subplot(323)\n",
        "plot_predictions([tree_reg2], X, y2, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$h_2(x_1)$\", style=\"g-\", data_style=\"k+\", data_label=\"Residuals\")\n",
        "plt.ylabel(\"$y - h_1(x_1)$\", fontsize=16)\n",
        "\n",
        "plt.subplot(324)\n",
        "plot_predictions([tree_reg1, tree_reg2], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1) + h_2(x_1)$\")\n",
        "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
        "\n",
        "plt.subplot(325)\n",
        "plot_predictions([tree_reg3], X, y3, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$h_3(x_1)$\", style=\"g-\", data_style=\"k+\")\n",
        "plt.ylabel(\"$y - h_1(x_1) - h_2(x_1)$\", fontsize=16)\n",
        "plt.xlabel(\"$x_1$\", fontsize=16)\n",
        "\n",
        "plt.subplot(326)\n",
        "plot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=16)\n",
        "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLnbZvQVEt8u"
      },
      "source": [
        "**Task 7c)**  \n",
        "We are also going to compare two gradient boosting regressor objects. <br>\n",
        "Both with `max_depth=2` and `random_state=42`, but one will be 'fast' and one <br>\n",
        "will be 'slow'. <br>\n",
        "For the fast one, set the learning rate to 1 and number of estimators to 3. <br>\n",
        "For the slow one, set the learning rate to 0.1 and number of estimators to 200."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJx1_0cJEt8u"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9N8YagE_Et8u"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "#gbrt_fast =\n",
        "#gbrt_slow ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxB0lQLeEt8u"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV3SE9j9Et8u"
      },
      "source": [
        "Next, we are fitting them to the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpsirEQCEt8u"
      },
      "outputs": [],
      "source": [
        "gbrt_fast.fit(X, y)\n",
        "gbrt_slow.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2AG7v_lEt8v"
      },
      "outputs": [],
      "source": [
        "fix, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True)\n",
        "\n",
        "plt.sca(axes[0])\n",
        "plot_predictions([gbrt_fast], X, y, axes=[-0.5, 0.5, -0.1, 0.8],\n",
        "                 label=\"Ensemble predictions\")\n",
        "plt.title(\"learning_rate={}, n_estimators={}\".format(gbrt_fast.learning_rate,\n",
        "                                                     gbrt_fast.n_estimators),\n",
        "          fontsize=14)\n",
        "plt.xlabel(\"$x_1$\", fontsize=16)\n",
        "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
        "\n",
        "plt.sca(axes[1])\n",
        "plot_predictions([gbrt_slow], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
        "plt.title(\"learning_rate={}, n_estimators={}\".format(gbrt_slow.learning_rate,\n",
        "                                                     gbrt_slow.n_estimators),\n",
        "          fontsize=14)\n",
        "plt.xlabel(\"$x_1$\", fontsize=16)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMqeIqIiEt8v"
      },
      "source": [
        "## Gradient Boosting with Early stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WLzQAl5Et8v"
      },
      "source": [
        "In this section, we are going to be looking for the lowest MSE in a set of <br> boosted trees. Just run the cells below and discuss the results with your group.\n",
        "\n",
        "**Again, the important thing here is to understand the plots. Don't worry about** <br>\n",
        "**trying to understand all of the code**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-NTRJQ0Et8v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=49)\n",
        "\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\n",
        "gbrt.fit(X_train, y_train)\n",
        "\n",
        "errors = [mean_squared_error(y_val, y_pred)\n",
        "          for y_pred in gbrt.staged_predict(X_val)]\n",
        "bst_n_estimators = np.argmin(errors) + 1\n",
        "\n",
        "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=42)\n",
        "gbrt_best.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlbA0vaGEt8v"
      },
      "outputs": [],
      "source": [
        "min_error = np.min(errors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZHmFWx7Et8v"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot(errors, \"b.-\")\n",
        "plt.plot([bst_n_estimators, bst_n_estimators], [0, min_error], \"k--\")\n",
        "plt.plot([0, 120], [min_error, min_error], \"k--\")\n",
        "plt.plot(bst_n_estimators, min_error, \"ko\")\n",
        "plt.text(bst_n_estimators, min_error*1.2, \"Minimum\", ha=\"center\", fontsize=14)\n",
        "plt.axis([0, 120, 0, 0.01])\n",
        "plt.xlabel(\"Number of trees\")\n",
        "plt.ylabel(\"Error\", fontsize=16)\n",
        "plt.title(\"Validation error\", fontsize=14)\n",
        "\n",
        "plt.subplot(122)\n",
        "plot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
        "plt.title(\"Best model (%d trees)\" % bst_n_estimators, fontsize=14)\n",
        "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
        "plt.xlabel(\"$x_1$\", fontsize=16)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKTn1rKZEt8w"
      },
      "outputs": [],
      "source": [
        "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)\n",
        "\n",
        "min_val_error = float(\"inf\")\n",
        "error_going_up = 0\n",
        "for n_estimators in range(1, 120):\n",
        "    gbrt.n_estimators = n_estimators\n",
        "    gbrt.fit(X_train, y_train)\n",
        "    y_pred = gbrt.predict(X_val)\n",
        "    val_error = mean_squared_error(y_val, y_pred)\n",
        "    if val_error < min_val_error:\n",
        "        min_val_error = val_error\n",
        "        error_going_up = 0\n",
        "    else:\n",
        "        error_going_up += 1\n",
        "        if error_going_up == 5:\n",
        "            break  # early stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3uuUhQvEt8w"
      },
      "outputs": [],
      "source": [
        "print(gbrt.n_estimators)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOk90onMEt8w"
      },
      "outputs": [],
      "source": [
        "print(\"Minimum validation MSE:\", min_val_error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F2ARIdKEt8w"
      },
      "source": [
        "## Using XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C4cxkagEt8w"
      },
      "source": [
        "### Task 8\n",
        "Finally, we move to an [XGBoost](https://xgboost.readthedocs.io/en/stable/) regressor - arguably one of the most popular and <br>\n",
        "widely-used algorithms nowadays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "offvY0Q-Et8y"
      },
      "source": [
        "First, make sure you have it installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7C_TB_kEt8y"
      },
      "outputs": [],
      "source": [
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za89zTFHEt8y"
      },
      "outputs": [],
      "source": [
        "import xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0aCXoFLEt8y"
      },
      "outputs": [],
      "source": [
        "xgb_reg = xgboost.XGBRegressor(random_state=42)\n",
        "xgb_reg.fit(X_train, y_train)\n",
        "y_pred = xgb_reg.predict(X_val)\n",
        "val_error = np.sqrt(mean_squared_error(y_val, y_pred)) # Not shown\n",
        "print(\"Validation MSE:\", val_error)           # Not shown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zJW274OEt8y"
      },
      "source": [
        "**Task 8**  \n",
        "Do the same thing as above but while fitting, add a validation sets and set <br>\n",
        "`early_stopping_rounds=2`. To do that, you want to add the `eval_set` parameter <br>\n",
        "and set `[(X_val, y_val)]` value there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yojQVgdEt8y"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cG8qWqjIEt8z"
      },
      "outputs": [],
      "source": [
        "val_errors_with_stop = []\n",
        "# fit\n",
        "# xgb_reg =\n",
        "# add the validation set and an early stopping, fit\n",
        "#\n",
        "val_errors_with_stop = xgb_reg.evals_result()['validation_0']['rmse']\n",
        "y_pred = xgb_reg.predict(X_val)\n",
        "val_error = np.sqrt(mean_squared_error(y_val, y_pred))  # Not shown\n",
        "print(\"Validation RMSE:\", val_error)            # Not shown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc4mLBV9Et8z"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "nav_menu": {
      "height": "252px",
      "width": "333px"
    },
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}